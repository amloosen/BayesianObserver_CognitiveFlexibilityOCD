{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719902b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def get_repo_root():\n",
    "    try:\n",
    "        # This command returns the absolute path of the repository root.\n",
    "        repo_root = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"--show-toplevel\"], stderr=subprocess.STDOUT\n",
    "        ).strip().decode(\"utf-8\")\n",
    "        return repo_root\n",
    "    except subprocess.CalledProcessError:\n",
    "        # If not in a git repository, fall back to current working directory.\n",
    "        return os.getcwd()\n",
    "\n",
    "repo_root = get_repo_root()\n",
    "print(\"Repository Root:\", repo_root)\n",
    "\n",
    "def read_csv_files(directory):\n",
    "    csv_files = []\n",
    "    print(f\"Checking files in directory: {directory}\")\n",
    "    for filename in os.listdir(directory):\n",
    "        print(f\"Found file: {filename}\")\n",
    "        if filename.startswith(\"sub\") and filename.endswith(\".csv\"):\n",
    "            print(f\"Reading CSV file: {filename}\")\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            csv_files.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping file: {filename}\")\n",
    "    return csv_files\n",
    "\n",
    "def summarySE(df, measurevar, groupvars, na_rm=True):\n",
    "    \"\"\"Compute mean, standard error, and count of observations for each group.\"\"\"\n",
    "    # Group by the specified columns\n",
    "    grouped = df.groupby(groupvars).agg(\n",
    "        mean=(measurevar, 'mean'),\n",
    "        count=(measurevar, 'size'),\n",
    "        std=(measurevar, 'std')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate standard error\n",
    "    grouped['se'] = grouped['std'] / np.sqrt(grouped['count'])\n",
    "    \n",
    "    # Remove rows with NaNs if na_rm is True\n",
    "    if na_rm:\n",
    "        grouped = grouped.dropna()\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "# Set the directory path\n",
    "directory = os.path.join(repo_root, \"data\")\n",
    "\n",
    "# Call the function and get the list of dataframes\n",
    "dataframes = read_csv_files(directory)\n",
    "\n",
    "# Display the first few rows of each DataFrame\n",
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"First few rows of DataFrame {i+1}:\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to the data folder within the repository.\n",
    "data_dir = os.path.join(repo_root, \"data\")\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Load data\n",
    "CrypticCreatures = pd.read_csv(\"Table_CrypticCreatures_YaleCohort.csv\")\n",
    "CrypticCreatures.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "CrypticCreature_relativeShift = pd.read_csv(\"Table_CrypticCreaturesShiftRelative_YaleCohort.csv\")\n",
    "CrypticCreatures_patients_relativeShift = pd.read_csv(\"Table_CrypticCreaturesShiftRelative_patients_YaleCohort.csv\")\n",
    "CrypticCreatures_patients_relativeShift = CrypticCreatures_patients_relativeShift.sort_values(by=['id', 'nTrial_rel'])\n",
    "CrypticCreatures_controls_relativeShift = pd.read_csv(\"Table_CrypticCreaturesShiftRelative_controls_YaleCohort.csv\")\n",
    "CrypticCreatures_controls_relativeShift = CrypticCreatures_controls_relativeShift.sort_values(by=['id', 'nTrial_rel'])\n",
    "\n",
    "CrypticCreatures_BayesianLearner = pd.read_csv(\"CrypticCreatures_BayesianLearner.csv\")\n",
    "CrypticCreatures_BayesianLearner = CrypticCreatures_BayesianLearner.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "CrypticCreatures_BayesianLearner_patients_relativeShift = pd.read_csv(\"CrypticCreaturesBayesianLearner_relativeShift_OCD.csv\")\n",
    "CrypticCreatures_BayesianLearner_patients_relativeShift = CrypticCreatures_BayesianLearner_patients_relativeShift.sort_values(by=['id', 'nTrial_rel'])\n",
    "CrypticCreatures_BayesianLearner_controls_relativeShift = pd.read_csv(\"CrypticCreaturesBayesianLearner_relativeShift_controls.csv\")\n",
    "CrypticCreatures_BayesianLearner_controls_relativeShift = CrypticCreatures_BayesianLearner_controls_relativeShift.sort_values(by=['id', 'nTrial_rel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04419d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming CrypticCreatures_patients_relativeShift is a DataFrame\n",
    "Cryptic_mean_acc_ID_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_accuracy_id', ['nTrial_rel'])\n",
    "Cryptic_mean_acc_ED_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_accuracy_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_acc_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_accuracy', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_conf_ID_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_confidence_id', ['nTrial_rel'])\n",
    "Cryptic_mean_conf_ED_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_confidence_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_conf_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_confidence', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_acc_ID_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_accuracy_id', ['nTrial_rel'])\n",
    "Cryptic_mean_acc_ED_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_accuracy_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_acc_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_accuracy', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_conf_ID_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_confidence_id', ['nTrial_rel'])\n",
    "Cryptic_mean_conf_ED_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_confidence_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_conf_controls= summarySE(CrypticCreatures_controls_relativeShift, 'mean_confidence', ['nTrial_rel'])\n",
    "\n",
    "#Cryptic_mean_entr_ID_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeID, 'mean_entropy_id', ['nTrial_rel'])\n",
    "#Cryptic_mean_entr_ED_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeED, 'mean_entropy_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_entr_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'entropy', ['nTrial_rel'])\n",
    "Cryptic_mean_entr_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'entropy', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_sumprior_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'sum_prior_chosen_features', ['nTrial_rel'])\n",
    "Cryptic_mean_sumprior_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'sum_prior_chosen_features', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_BLR_confidence_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'BLR_confidence', ['nTrial_rel'])\n",
    "Cryptic_mean_BLR_confidence_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'BLR_confidence', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_signed_confidence_deviation_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'signed_confidence_deviation', ['nTrial_rel'])\n",
    "Cryptic_mean_signed_confidence_deviation_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'signed_confidence_deviation', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_signed_prior_deviation_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'signed_prior_deviation', ['nTrial_rel'])\n",
    "Cryptic_mean_signed_prior_deviation_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'signed_prior_deviation', ['nTrial_rel'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95193199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a column for feedback type (1 for correct, 0 for incorrect)\n",
    "# CrypticCreatures['feedback'] = CrypticCreatures['chosen_outcome'].apply(lambda x: 1 if x == 'correct' else 0)\n",
    "\n",
    "# # Create lagged columns for feedback and confidence\n",
    "# CrypticCreatures['prev_feedback'] = CrypticCreatures['feedback'].shift(1)\n",
    "# CrypticCreatures['prev_confidence'] = CrypticCreatures['confidence'].shift(1)\n",
    "\n",
    "# # Filter out the first trial as it has no previous feedback\n",
    "# CrypticCreatures = CrypticCreatures.dropna(subset=['prev_feedback', 'prev_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summary_rel(df, measurevar, groupvar):\n",
    "    \"\"\"\n",
    "    Calculate mean, standard error, and count for a given measure variable.\n",
    "    \"\"\"\n",
    "    summary = df.groupby(groupvar).agg(\n",
    "        mean=(measurevar, 'mean'),\n",
    "        count=(measurevar, 'size'),\n",
    "        std=(measurevar, 'std')\n",
    "    ).reset_index()\n",
    "    summary['stderr'] = summary['std'] / np.sqrt(summary['count'])\n",
    "    return summary\n",
    "\n",
    "# data frames\n",
    "controls_df = CrypticCreatures_controls_relativeShift \n",
    "patients_df = CrypticCreatures_patients_relativeShift\n",
    "\n",
    "# Merge controls dataset based on 'nTrial_rel' and 'id'\n",
    "controls_merged_df = pd.merge(\n",
    "    CrypticCreatures_BayesianLearner_controls_relativeShift,\n",
    "    controls_df,\n",
    "    on=['nTrial_rel', 'id'],  # Merge on both nTrial_rel and id\n",
    "    how='inner'  # Use inner join to ensure matching nTrial_rel and id\n",
    ")\n",
    "\n",
    "# Merge patients dataset based on 'nTrial_rel' and 'id'\n",
    "patients_merged_df = pd.merge(\n",
    "    CrypticCreatures_BayesianLearner_patients_relativeShift,\n",
    "    patients_df,\n",
    "    on=['nTrial_rel', 'id'],  # Merge on both nTrial_rel and id\n",
    "    how='inner'  # Use inner join to ensure matching nTrial_rel and id\n",
    ")\n",
    "# List of measure variables for controls and patients\n",
    "measure_vars = [\n",
    "    'change_in_mean_accuracy', 'change_in_mean_accuracy_abs', \n",
    "    'change_in_mean_accuracy_ed', 'change_in_mean_accuracy_abs_ed', \n",
    "    'change_in_mean_accuracy_id', 'change_in_mean_accuracy_abs_id',\n",
    "    'change_in_mean_confidence', 'change_in_mean_confidence_abs',\n",
    "    'change_in_mean_confidence_ed', 'change_in_mean_confidence_abs_ed',\n",
    "    'change_in_mean_confidence_id', 'change_in_mean_confidence_abs_id',\n",
    "    'signed_confidence_deviation','signed_prior_deviation','mean_confidence','BLR_confidence',\n",
    "    'mean_accuracy'\n",
    "    \n",
    "]\n",
    "\n",
    "# Calculate summaries for controls and patients\n",
    "summary_controls_rel = {var: calculate_summary_rel(controls_merged_df, var, 'nTrial_rel') for var in measure_vars}\n",
    "summary_patients_rel = {var: calculate_summary_rel(patients_merged_df, var, 'nTrial_rel') for var in measure_vars}\n",
    "print(summary_controls_rel['mean_confidence'])\n",
    "print(Cryptic_mean_conf_controls)\n",
    "# \n",
    "summary_controls_accuracy = summary_controls_rel['change_in_mean_accuracy']\n",
    "summary_patients_accuracy = summary_patients_rel['change_in_mean_accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de506c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def summarySE(data, measurevar, groupvars, na_rm=True):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a given measure variable.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame\n",
    "    - measurevar: str, the name of the measure variable\n",
    "    - groupvars: list of str, the names of the grouping variables\n",
    "    - na_rm: bool, whether to remove NA values\n",
    "    \n",
    "    Returns:\n",
    "    - summary: DataFrame with summary statistics\n",
    "    \"\"\"\n",
    "    # Remove NA values if na_rm is True\n",
    "    if na_rm:\n",
    "        data = data.dropna(subset=[measurevar])\n",
    "    \n",
    "    # Group by the specified variables and compute the summary statistics\n",
    "    summary = data.groupby(groupvars).agg(\n",
    "        mean=(measurevar, 'mean'),\n",
    "        count=(measurevar, 'size'),\n",
    "        std=(measurevar, 'std')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Compute the standard error\n",
    "    summary['se'] = summary['std'] / np.sqrt(summary['count'])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Assuming CrypticCreatures_controls_relativeShift and CrypticCreatures_patients_relativeShift are defined as DataFrames\n",
    "\n",
    "# Compute summary statistics for controls\n",
    "Cryptic_mean_confidence_controls = summarySE(\n",
    "    CrypticCreatures_controls_relativeShift, \n",
    "    measurevar=\"mean_confidence\", \n",
    "    groupvars=[\"nTrial_rel\"],\n",
    "    na_rm=True\n",
    ")\n",
    "\n",
    "# Compute summary statistics for patients\n",
    "Cryptic_mean_confidence_patients = summarySE(\n",
    "    CrypticCreatures_patients_relativeShift, \n",
    "    measurevar=\"mean_confidence\", \n",
    "    groupvars=[\"nTrial_rel\"],\n",
    "    na_rm=True\n",
    ")\n",
    "\n",
    "# Print the results to compare\n",
    "print(\"Summary statistics for controls:\")\n",
    "print(Cryptic_mean_confidence_controls)\n",
    "\n",
    "print(\"\\nSummary statistics for patients:\")\n",
    "print(Cryptic_mean_confidence_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27760954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, shapiro, levene, mannwhitneyu\n",
    "\n",
    "# Assuming CrypticCreatures is your dataset\n",
    "# CrypticCreatures = pd.read_csv('path_to_CrypticCreatures.csv')\n",
    "\n",
    "# Compute each individual's average accuracy and confidence\n",
    "CrypticCreatures['average_accuracy'] = CrypticCreatures.groupby('id')['chosen_outcome'].transform('mean')\n",
    "CrypticCreatures['average_confidence'] = CrypticCreatures.groupby('id')['confidence'].transform('mean')\n",
    "average_data = CrypticCreatures[['id', 'average_accuracy', 'average_confidence', 'patientstatus']].drop_duplicates()\n",
    "\n",
    "# Separate the groups\n",
    "controls_acc = average_data[average_data['patientstatus'] == 0]['average_accuracy']\n",
    "patients_acc = average_data[average_data['patientstatus'] == 1]['average_accuracy']\n",
    "controls_conf = average_data[average_data['patientstatus'] == 0]['average_confidence']\n",
    "patients_conf = average_data[average_data['patientstatus'] == 1]['average_confidence']\n",
    "\n",
    "# Function to perform tests and return p-value\n",
    "def perform_tests(controls, patients, measure_name):\n",
    "    # Check for normality\n",
    "    shapiro_controls = shapiro(controls)\n",
    "    shapiro_patients = shapiro(patients)\n",
    "    print(f'Shapiro-Wilk Test for Controls {measure_name}: {shapiro_controls}')\n",
    "    print(f'Shapiro-Wilk Test for Patients {measure_name}: {shapiro_patients}')\n",
    "    \n",
    "    # Check for homogeneity of variances\n",
    "    levene_test = levene(controls, patients)\n",
    "    print(f'Levene\\'s Test for {measure_name}: {levene_test}')\n",
    "    \n",
    "    # Perform t-test if assumptions are met, otherwise use Mann-Whitney U test\n",
    "    if shapiro_controls.pvalue > 0.05 and shapiro_patients.pvalue > 0.05 and levene_test.pvalue > 0.05:\n",
    "        t_stat, p_value = ttest_ind(controls, patients)\n",
    "        print(f'T-test for {measure_name}: T-statistic = {t_stat}, P-value = {p_value}')\n",
    "    else:\n",
    "        u_stat, p_value = mannwhitneyu(controls, patients)\n",
    "        print(f'Mann-Whitney U Test for {measure_name}: U-statistic = {u_stat}, P-value = {p_value}')\n",
    "    return p_value\n",
    "\n",
    "# Perform tests and get p-values\n",
    "p_value_acc = perform_tests(controls_acc, patients_acc, 'Accuracy')\n",
    "p_value_conf = perform_tests(controls_conf, patients_conf, 'Confidence')\n",
    "\n",
    "# Function to draw significance bracket\n",
    "def draw_significance_bracket(ax, x1, x2, y, text, height_percent=0.02):\n",
    "    height = y * height_percent\n",
    "    ax.plot([x1, x1, x2, x2], [y, y + height, y + height, y], lw=1.5, color='black')\n",
    "    ax.plot([x1, x1], [y, y - height], lw=1.5, color='black')  # Left vertical line\n",
    "    ax.plot([x2, x2], [y, y - height], lw=1.5, color='black')  # Right vertical line\n",
    "    ax.text((x1 + x2) * 0.5, y + height * 1.5, text, ha='center', va='bottom', color='black', fontsize=20)\n",
    "\n",
    "# Determine significance text based on p-value\n",
    "def get_significance_text(p_value):\n",
    "    if p_value < 0.001:\n",
    "        return '***'\n",
    "    elif p_value < 0.01:\n",
    "        return '**'\n",
    "    elif p_value < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'n.s.'\n",
    "\n",
    "# Plotting function for accuracy\n",
    "def plot_accuracy_boxplot(average_data, p_value_acc):\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    ax = sns.boxplot(x='patientstatus', y='average_accuracy', data=average_data, palette={0: '#4F67B1' , 1: '#B44B28'})\n",
    "    sns.swarmplot(x='patientstatus', y='average_accuracy', data=average_data, color='black', dodge=True, ax=ax, marker='o', size=5)\n",
    "    \n",
    "    # Add significance indication for accuracy\n",
    "    max_acc = max(average_data['average_accuracy'])\n",
    "    ylim = max_acc + 0.1\n",
    "    significance_text = get_significance_text(p_value_acc)\n",
    "    draw_significance_bracket(ax, 0, 1, ylim - 0.07, significance_text)\n",
    "    \n",
    "    ax.set_ylim(0.4, ylim)\n",
    "    ax.set_xticklabels(['Controls', 'Patients'])\n",
    "    ax.set_yticks(np.arange(0.5, 0.9, 0.1))\n",
    "    ax.set_title('Average Accuracy Comparison')\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_ylabel('Average Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting function for confidence\n",
    "def plot_confidence_boxplot(average_data, p_value_conf):\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    ax = sns.boxplot(x='patientstatus', y='average_confidence', data=average_data, palette={0: '#4F67B1' , 1: '#B44B28'})\n",
    "    sns.swarmplot(x='patientstatus', y='average_confidence', data=average_data, color='black', dodge=True, ax=ax, marker='o', size=5)\n",
    "    \n",
    "    # Add significance indication for confidence\n",
    "    max_conf = max(average_data['average_confidence'])\n",
    "    ylim = max_conf + 10\n",
    "    significance_text = get_significance_text(p_value_conf)\n",
    "    draw_significance_bracket(ax, 0, 1, ylim - 7, significance_text)\n",
    "    \n",
    "    ax.set_ylim(0, ylim)\n",
    "    ax.set_xticklabels(['Controls', 'Patients'])\n",
    "    ax.set_yticks(np.arange(0, 121, 20))\n",
    "    ax.set_title('Average Confidence Comparison')\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_ylabel('Average Confidence')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the boxplots separately\n",
    "plot_accuracy_boxplot(average_data, p_value_acc)\n",
    "plot_confidence_boxplot(average_data, p_value_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146128c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_var(summary_controls, summary_patients, measurevar, title, colors):\n",
    "    \"\"\"\n",
    "    Plot the change in mean var across shifts for controls and patients.\n",
    "    \"\"\"\n",
    "    # Add group labels\n",
    "    summary_controls['patientstatus'] = 0\n",
    "    summary_patients['patientstatus'] = 1\n",
    "    \n",
    "    # Combine datasets\n",
    "    df_combined = pd.concat([summary_controls, summary_patients], ignore_index=True)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for group in [0, 1]:\n",
    "        subset = df_combined[df_combined['patientstatus'] == group]\n",
    "        \n",
    "        # Plot line and scatter points\n",
    "        sns.lineplot(data=subset, x='nTrial_rel', y='mean', color=colors[group])\n",
    "        sns.scatterplot(data=subset, x='nTrial_rel', y='mean', color=colors[group], edgecolor=colors[group], s=100, label='Controls' if group == 0 else 'Patients')\n",
    "\n",
    "        # Add error bars\n",
    "        plt.errorbar(subset['nTrial_rel'], subset['mean'], yerr=subset['stderr'], fmt='o', color=colors[group], capsize=5)\n",
    "    \n",
    "    #plt.axhline(0, color='black', linewidth=1.2, linestyle='--')\n",
    "    plt.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Trial (0=Shifts)')\n",
    "    plt.ylabel('Mean Accuracy')\n",
    "    plt.legend(title='Group')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Optionally, update the global rcParams to set the minimum font size:\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# ============\n",
    "# (1) Adapted Plot Functions that accept an axis\n",
    "# ============\n",
    "def draw_significance_bracket(ax, x1, x2, y, text, height_percent=0.01):\n",
    "    height = y * height_percent\n",
    "    ax.plot([x1, x1, x2, x2], [y, y + height, y + height, y], lw=1.5, color='black')\n",
    "    ax.plot([x1, x1], [y, y - height], lw=1.5, color='black')  # Left vertical line\n",
    "    ax.plot([x2, x2], [y, y - height], lw=1.5, color='black')  # Right vertical line\n",
    "    # Significance text is set to 16 (min)\n",
    "    ax.text((x1 + x2) * 0.5, y + height * 1.5, text, ha='center', va='bottom', \n",
    "            color='black', fontsize=16)\n",
    "\n",
    "def get_significance_text(p_value):\n",
    "    if p_value < 0.001:\n",
    "        return '***'\n",
    "    elif p_value < 0.01:\n",
    "        return '**'\n",
    "    elif p_value < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'n.s.'\n",
    "    \n",
    "def plot_accuracy_boxplot_ax(ax, average_data, p_value_acc):\n",
    "    # Boxplot and swarmplot:\n",
    "    sns.boxplot(x='patientstatus', y='average_accuracy', data=average_data, \n",
    "                palette={0: '#4F67B1' , 1: '#B44B28'}, ax=ax, width=0.3, linewidth=1.5)\n",
    "    sns.stripplot(x='patientstatus', y='average_accuracy', data=average_data, \n",
    "                  color='#222328', jitter=True, ax=ax, marker='o', size=4)\n",
    "    # Determine y-limit and add significance bracket:\n",
    "    max_acc = average_data['average_accuracy'].max()\n",
    "    ylim = max_acc + 0.1\n",
    "    significance_text = get_significance_text(p_value_acc)\n",
    "    draw_significance_bracket(ax, 0, 1, ylim - 0.07, significance_text)\n",
    "    \n",
    "    # Format the axis:\n",
    "    ax.set_ylim(0.4, ylim)\n",
    "    ax.set_xticks([0.5])\n",
    "    ax.set_xticklabels(['Overall'])\n",
    "    ax.set_yticks(np.arange(0.5, 0.9, 0.1))\n",
    "    ax.set_xlabel(' ')\n",
    "    # Increase y-label font size to 18:\n",
    "    ax.set_ylabel('Average Accuracy', fontweight='bold', fontsize=16)\n",
    "    # Set tick label size to at least 16:\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "    \n",
    "    sns.despine()\n",
    "\n",
    "def plot_confidence_boxplot_ax(ax, average_data, p_value_conf):\n",
    "    sns.boxplot(x='patientstatus', y='average_confidence', data=average_data, \n",
    "                palette={0: '#4F67B1' , 1: '#B44B28'}, ax=ax, width=0.3,linewidth=1.5)\n",
    "    sns.stripplot(x='patientstatus', y='average_confidence', data=average_data, \n",
    "                  color='#222328', jitter=True, ax=ax, marker='o', size=4)\n",
    "    \n",
    "    max_conf = average_data['average_confidence'].max()\n",
    "    ylim = max_conf + 10\n",
    "    significance_text = get_significance_text(p_value_conf)\n",
    "    draw_significance_bracket(ax, 0, 1, ylim - 7, significance_text)\n",
    "    \n",
    "    ax.set_ylim(0, ylim)\n",
    "    ax.set_xticks([0.5])\n",
    "    ax.set_xticklabels(['Overall'])\n",
    "    ax.set_yticks(np.arange(0, 121, 20))\n",
    "    ax.set_xlabel(' ')\n",
    "    ax.set_ylabel('Average Confidence', fontweight='bold', fontsize=16)\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "    \n",
    "    sns.despine()\n",
    "\n",
    "def plot_mean_var_ax(ax, summary_controls, summary_patients, measurevar, ylabel, xlabel, ylim, yticks, colors):\n",
    "    # Convert inputs to DataFrames and add a group label:\n",
    "    summary_controls = pd.DataFrame(summary_controls)\n",
    "    summary_patients = pd.DataFrame(summary_patients)\n",
    "    summary_controls['patientstatus'] = 'Controls'\n",
    "    summary_patients['patientstatus'] = 'Patients'\n",
    "    \n",
    "    df_combined = pd.concat([summary_controls, summary_patients], ignore_index=True)\n",
    "    ax.axvline(0, color='black', linewidth=1, linestyle='--')\n",
    "    # Plot lines, scatter points, and error bars:\n",
    "    for group_key, color in colors.items():\n",
    "        label = 'Controls' if group_key == 0 else 'Patients'\n",
    "        subset = df_combined[df_combined['patientstatus'] == label]\n",
    "        \n",
    "        sns.lineplot(data=subset, x='nTrial_rel', y='mean', color=color, ax=ax)\n",
    "        sns.scatterplot(data=subset, x='nTrial_rel', y='mean', color=color, \n",
    "                        edgecolor='white', s=100, label=label, ax=ax)\n",
    "        ax.errorbar(subset['nTrial_rel'], subset['mean'], yerr=subset['stderr'], \n",
    "                    fmt='none', color=color, capsize=5)\n",
    "    \n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_xticks(np.arange(-5, 6, 1))\n",
    "    \n",
    "    # Increase x- and y-label font sizes to 18:\n",
    "    ax.set_xlabel(' ', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel(ylabel, fontsize=16)\n",
    "    ax.set_xlabel(xlabel, fontsize=16)\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "    # Update legend font sizes:\n",
    "    if measurevar == 'mean_accuracy':\n",
    "        ax.legend(title='Group', fontsize=14, title_fontsize=14, loc='lower left')\n",
    "    else:\n",
    "        legend = ax.get_legend()\n",
    "        if legend is not None:\n",
    "            legend.remove()\n",
    "    sns.despine()\n",
    "\n",
    "# ============\n",
    "# (2) Create the Combined Figure\n",
    "# ============\n",
    "# (Note: average_data, p_value_acc, p_value_conf, summary_controls_rel, \n",
    "# and summary_patients_rel should be defined in your workspace.)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    2, 2, \n",
    "    figsize=(10, 10), \n",
    "    sharey='row', \n",
    "    gridspec_kw={'width_ratios': [1, 2], 'wspace': 0}\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(wspace=0)\n",
    "\n",
    "# Plot accuracy boxplot and mean variance plot:\n",
    "plot_accuracy_boxplot_ax(axes[0, 0], average_data, p_value_acc)\n",
    "plot_mean_var_ax(axes[0, 1], \n",
    "                 summary_controls_rel['mean_accuracy'], \n",
    "                 summary_patients_rel['mean_accuracy'], \n",
    "                 'mean_accuracy', 'Mean Accuracy',' ',\n",
    "                 (0, 1), np.arange(0, 1.1, 0.2), \n",
    "                 colors={0: '#4F67B1' , 1: '#B44B28'})\n",
    "\n",
    "# Add 'A' label to the upper left corner of the first row of plots\n",
    "axes[0, 0].text(-0.1, 1.1, 'A', transform=axes[0, 0].transAxes, \n",
    "                fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "\n",
    "# Plot confidence boxplot and mean variance plot:\n",
    "plot_confidence_boxplot_ax(axes[1, 0], average_data, p_value_conf)\n",
    "plot_mean_var_ax(axes[1, 1], \n",
    "                 summary_controls_rel['mean_confidence'], \n",
    "                 summary_patients_rel['mean_confidence'], \n",
    "                 'mean_confidence', 'Mean Confidence', 'Trial (0=Shifts)',\n",
    "                 (20, 115), np.arange(20, 110, 20), \n",
    "                 colors={0: '#4F67B1' , 1: '#B44B28'})\n",
    "\n",
    "# Add 'B' label to the upper left corner of the second row of plots\n",
    "axes[1, 0].text(-0.1, 1.1, 'B', transform=axes[1, 0].transAxes, \n",
    "                fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Create the subfolder if it doesn't exist\n",
    "if not os.path.exists('../figures'):\n",
    "    os.makedirs('../figures')\n",
    "\n",
    "# Save the figure into the subfolder\n",
    "plt.savefig('../figures/Fig2_AccConf_Average.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa52e962",
   "metadata": {},
   "source": [
    "Below are plots to look at changes in variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Statistical Test Section for \"After Incorrect Trials\" ---\n",
    "if len(after_incorrect_controls) > 0 and len(after_incorrect_patients) > 0:\n",
    "    # --- Remove outliers in after_incorrect_controls using IQR ---\n",
    "    Q1_controls = after_incorrect_controls.quantile(0.25)\n",
    "    Q3_controls = after_incorrect_controls.quantile(0.75)\n",
    "    IQR_controls = Q3_controls - Q1_controls\n",
    "    lower_bound_controls = Q1_controls - 3 * IQR_controls\n",
    "    upper_bound_controls = Q3_controls + 3 * IQR_controls\n",
    "    filtered_controls = after_incorrect_controls[(after_incorrect_controls >= lower_bound_controls) & \n",
    "                                                  (after_incorrect_controls <= upper_bound_controls)]\n",
    "    \n",
    "    # --- Remove outliers in after_incorrect_patients using IQR ---\n",
    "    Q1_patients = after_incorrect_patients.quantile(0.25)\n",
    "    Q3_patients = after_incorrect_patients.quantile(0.75)\n",
    "    IQR_patients = Q3_patients - Q1_patients\n",
    "    lower_bound_patients = Q1_patients - 3 * IQR_patients\n",
    "    upper_bound_patients = Q3_patients + 3 * IQR_patients\n",
    "    filtered_patients = after_incorrect_patients[(after_incorrect_patients >= lower_bound_patients) & \n",
    "                                                  (after_incorrect_patients <= upper_bound_patients)]\n",
    "    \n",
    "    # Check that both filtered groups have data\n",
    "    if len(filtered_controls) > 0 and len(filtered_patients) > 0:\n",
    "        # Check normality\n",
    "        shapiro_controls = shapiro(filtered_controls)\n",
    "        shapiro_patients = shapiro(filtered_patients)\n",
    "        # Check variance homogeneity\n",
    "        levene_test = levene(filtered_controls, filtered_patients)\n",
    "\n",
    "        print(\"\\nAfter Incorrect Trials Assumptions (after outlier removal):\")\n",
    "        print(f\"  Shapiro p-values: Controls={shapiro_controls.pvalue:.3f}, Patients={shapiro_patients.pvalue:.3f}\")\n",
    "        print(f\"  Levene p-value: {levene_test.pvalue:.3f}\")\n",
    "\n",
    "        if (shapiro_controls.pvalue > 0.05 and shapiro_patients.pvalue > 0.05) and (levene_test.pvalue > 0.05):\n",
    "            # Perform t-test\n",
    "            tstat_incorrect, pval_incorrect = ttest_ind(filtered_controls, filtered_patients)\n",
    "            print(\"After Incorrect Trials: t-test used\")\n",
    "            print(\"t = {:.3f}, p = {:.3f}\".format(tstat_incorrect, pval_incorrect))\n",
    "            significance_incorrect = pval_incorrect\n",
    "        else:\n",
    "            # Use nonparametric test\n",
    "            u_stat, pval_incorrect = mannwhitneyu(filtered_controls, filtered_patients, alternative='two-sided')\n",
    "            print(\"After Incorrect Trials: Mann-Whitney U test used\")\n",
    "            print(\"U = {:.3f}, p = {:.3f}\".format(u_stat, pval_incorrect))\n",
    "            significance_incorrect = pval_incorrect\n",
    "    else:\n",
    "        print(\"Insufficient data for testing after incorrect trials after outlier removal.\")\n",
    "else:\n",
    "    print(\"Insufficient data for testing after incorrect trials.\")\n",
    "\n",
    "# --- Plotting Section (Excluding Outliers) ---\n",
    "\n",
    "# Filter aggregated (per-participant) data for \"after_correct\"\n",
    "df_correct = avg_change_pivot[['id', 'after_correct', 'patientstatus']].dropna(subset=['after_correct'])\n",
    "Q1_correct = df_correct['after_correct'].quantile(0.25)\n",
    "Q3_correct = df_correct['after_correct'].quantile(0.75)\n",
    "IQR_correct = Q3_correct - Q1_correct\n",
    "lower_bound_correct = Q1_correct - 3 * IQR_correct\n",
    "upper_bound_correct = Q3_correct + 3 * IQR_correct\n",
    "df_correct_filtered = df_correct[(df_correct['after_correct'] >= lower_bound_correct) & \n",
    "                                 (df_correct['after_correct'] <= upper_bound_correct)]\n",
    "\n",
    "# Filter aggregated (per-participant) data for \"after_incorrect\"\n",
    "df_incorrect = avg_change_pivot[['id', 'after_incorrect', 'patientstatus']].dropna(subset=['after_incorrect'])\n",
    "Q1_incorrect = df_incorrect['after_incorrect'].quantile(0.25)\n",
    "Q3_incorrect = df_incorrect['after_incorrect'].quantile(0.75)\n",
    "IQR_incorrect = Q3_incorrect - Q1_incorrect\n",
    "lower_bound_incorrect = Q1_incorrect - 3 * IQR_incorrect\n",
    "upper_bound_incorrect = Q3_incorrect + 3 * IQR_incorrect\n",
    "df_incorrect_filtered = df_incorrect[(df_incorrect['after_incorrect'] >= lower_bound_incorrect) & \n",
    "                                     (df_incorrect['after_incorrect'] <= upper_bound_incorrect)]\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "# Left plot: After Correct Trials (without outliers)\n",
    "sns.boxplot(\n",
    "    data=df_correct_filtered,\n",
    "    x='patientstatus', y='after_correct',\n",
    "    palette=[\"#4F67B1\", \"#B44B28\"],\n",
    "    ax=axes[0],\n",
    "    showfliers=False\n",
    ")\n",
    "sns.stripplot(\n",
    "    data=df_correct_filtered,\n",
    "    x='patientstatus', y='after_correct',\n",
    "    color=\"#222328\",\n",
    "    size=5,\n",
    "    jitter=True,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].axhline(y=0, linestyle=\"--\", color=\"#333333\", linewidth=1)\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[0].set_ylabel(\"Average Confidence Change\\nAfter Correct Trials\", fontsize=16, fontweight=\"bold\")\n",
    "axes[0].set_xticklabels([\" \", \" \"], fontsize=14)\n",
    "axes[0].tick_params(axis='y', labelsize=14)\n",
    "axes[0].tick_params(axis='x', bottom=False)\n",
    "sns.despine(ax=axes[0])\n",
    "axes[0].text(0.02, 1.03, \"A\", transform=axes[0].transAxes,\n",
    "             fontsize=20, fontweight=\"bold\", va=\"bottom\", ha=\"left\")\n",
    "\n",
    "# Right plot: After Incorrect Trials (without outliers)\n",
    "sns.boxplot(\n",
    "    data=df_incorrect_filtered,\n",
    "    x='patientstatus', y='after_incorrect',\n",
    "    palette=[\"#4F67B1\", \"#B44B28\"],\n",
    "    ax=axes[1],\n",
    "    showfliers=False\n",
    ")\n",
    "sns.stripplot(\n",
    "    data=df_incorrect_filtered,\n",
    "    x='patientstatus', y='after_incorrect',\n",
    "    color=\"#222328\",\n",
    "    size=5,\n",
    "    jitter=True,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].axhline(y=0, linestyle=\"--\", color=\"#333333\", linewidth=1)\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[1].set_ylabel(\"Average Confidence Change\\nAfter Incorrect Trials\", fontsize=16, fontweight=\"bold\")\n",
    "axes[1].set_xticklabels([\" \", \" \"], fontsize=14)\n",
    "axes[1].tick_params(axis='y', labelsize=14)\n",
    "axes[1].tick_params(axis='x', bottom=False)\n",
    "sns.despine(ax=axes[1])\n",
    "axes[1].text(0.02, 1.03, \"B\", transform=axes[1].transAxes,\n",
    "             fontsize=20, fontweight=\"bold\", va=\"bottom\", ha=\"left\")\n",
    "\n",
    "# Create legend handles\n",
    "control_patch = mlines.Line2D([], [], color=\"#4F67B1\", marker=\"s\", linestyle=\"None\", markersize=10, label=\"Control\")\n",
    "patient_patch = mlines.Line2D([], [], color=\"#B44B28\", marker=\"s\", linestyle=\"None\", markersize=10, label=\"Patient\")\n",
    "axes[0].legend(handles=[control_patch, patient_patch], title='Group', fontsize=14, title_fontsize=14, loc='lower left')\n",
    "\n",
    "# Ensure both plots share the same y-limits so the zero-line is aligned\n",
    "common_ylim = [min(axes[0].get_ylim()[0], axes[1].get_ylim()[0]),\n",
    "               max(axes[0].get_ylim()[1], axes[1].get_ylim()[1]) + 2]\n",
    "axes[0].set_ylim(common_ylim)\n",
    "axes[1].set_ylim(common_ylim)\n",
    "\n",
    "# --- Add significance brackets using the helper functions ---\n",
    "if significance_correct is not None:\n",
    "    sig_text = get_significance_text(significance_correct)\n",
    "    y_max = df_correct_filtered['after_correct'].max() + 0.15\n",
    "    draw_significance_bracket(axes[0], 0, 1, y_max, sig_text)\n",
    "    \n",
    "if significance_incorrect is not None:\n",
    "    sig_text = get_significance_text(significance_incorrect)\n",
    "    y_max = df_incorrect_filtered['after_incorrect'].max() * 1.1\n",
    "    draw_significance_bracket(axes[1], 0, 1, y_max, sig_text)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def get_repo_root():\n",
    "    try:\n",
    "        # This command returns the absolute path of the repository root.\n",
    "        repo_root = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"--show-toplevel\"], stderr=subprocess.STDOUT\n",
    "        ).strip().decode(\"utf-8\")\n",
    "        return repo_root\n",
    "    except subprocess.CalledProcessError:\n",
    "        # If not in a git repository, fall back to current working directory.\n",
    "        return os.getcwd()\n",
    "\n",
    "repo_root = get_repo_root()\n",
    "print(\"Repository Root:\", repo_root)\n",
    "\n",
    "def read_csv_files(directory):\n",
    "    csv_files = []\n",
    "    print(f\"Checking files in directory: {directory}\")\n",
    "    for filename in os.listdir(directory):\n",
    "        print(f\"Found file: {filename}\")\n",
    "        if filename.startswith(\"sub\") and filename.endswith(\".csv\"):\n",
    "            print(f\"Reading CSV file: {filename}\")\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            csv_files.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping file: {filename}\")\n",
    "    return csv_files\n",
    "\n",
    "def summarySE(df, measurevar, groupvars, na_rm=True):\n",
    "    \"\"\"Compute mean, standard error, and count of observations for each group.\"\"\"\n",
    "    # Group by the specified columns\n",
    "    grouped = df.groupby(groupvars).agg(\n",
    "        mean=(measurevar, 'mean'),\n",
    "        count=(measurevar, 'size'),\n",
    "        std=(measurevar, 'std')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate standard error\n",
    "    grouped['se'] = grouped['std'] / np.sqrt(grouped['count'])\n",
    "    \n",
    "    # Remove rows with NaNs if na_rm is True\n",
    "    if na_rm:\n",
    "        grouped = grouped.dropna()\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "# Set the directory path\n",
    "directory = os.path.join(repo_root, \"data\")\n",
    "\n",
    "# Call the function and get the list of dataframes\n",
    "dataframes = read_csv_files(directory)\n",
    "\n",
    "# Display the first few rows of each DataFrame\n",
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"First few rows of DataFrame {i+1}:\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to the data folder within the repository.\n",
    "data_dir = os.path.join(repo_root, \"data\")\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Load data\n",
    "CrypticCreatures = pd.read_csv(\"Table_CrypticCreatures_YaleCohort.csv\")\n",
    "CrypticCreatures.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "CrypticCreature_relativeShift = pd.read_csv(\"Table_CrypticCreaturesShiftRelative_YaleCohort.csv\")\n",
    "CrypticCreatures_patients_relativeShift = pd.read_csv(\"Table_CrypticCreaturesShiftRelative_patients_YaleCohort.csv\")\n",
    "CrypticCreatures_patients_relativeShift = CrypticCreatures_patients_relativeShift.sort_values(by=['id', 'nTrial_rel'])\n",
    "CrypticCreatures_controls_relativeShift = pd.read_csv(\"Table_CrypticCreaturesShiftRelative_controls_YaleCohort.csv\")\n",
    "CrypticCreatures_controls_relativeShift = CrypticCreatures_controls_relativeShift.sort_values(by=['id', 'nTrial_rel'])\n",
    "\n",
    "CrypticCreatures_BayesianLearner = pd.read_csv(\"CrypticCreatures_BayesianLearner.csv\")\n",
    "CrypticCreatures_BayesianLearner = CrypticCreatures_BayesianLearner.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "CrypticCreatures_BayesianLearner_patients_relativeShift = pd.read_csv(\"CrypticCreaturesBayesianLearner_relativeShift_OCD.csv\")\n",
    "CrypticCreatures_BayesianLearner_patients_relativeShift = CrypticCreatures_BayesianLearner_patients_relativeShift.sort_values(by=['id', 'nTrial_rel'])\n",
    "CrypticCreatures_BayesianLearner_controls_relativeShift = pd.read_csv(\"CrypticCreaturesBayesianLearner_relativeShift_controls.csv\")\n",
    "CrypticCreatures_BayesianLearner_controls_relativeShift = CrypticCreatures_BayesianLearner_controls_relativeShift.sort_values(by=['id', 'nTrial_rel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming CrypticCreatures_patients_relativeShift is a DataFrame\n",
    "Cryptic_mean_acc_ID_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_accuracy_id', ['nTrial_rel'])\n",
    "Cryptic_mean_acc_ED_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_accuracy_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_acc_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_accuracy', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_conf_ID_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_confidence_id', ['nTrial_rel'])\n",
    "Cryptic_mean_conf_ED_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_confidence_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_conf_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_confidence', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_acc_ID_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_accuracy_id', ['nTrial_rel'])\n",
    "Cryptic_mean_acc_ED_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_accuracy_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_acc_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_accuracy', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_conf_ID_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_confidence_id', ['nTrial_rel'])\n",
    "Cryptic_mean_conf_ED_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_confidence_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_conf_controls= summarySE(CrypticCreatures_controls_relativeShift, 'mean_confidence', ['nTrial_rel'])\n",
    "\n",
    "#Cryptic_mean_entr_ID_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeID, 'mean_entropy_id', ['nTrial_rel'])\n",
    "#Cryptic_mean_entr_ED_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeED, 'mean_entropy_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_entr_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'entropy', ['nTrial_rel'])\n",
    "Cryptic_mean_entr_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'entropy', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_sumprior_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'sum_prior_chosen_features', ['nTrial_rel'])\n",
    "Cryptic_mean_sumprior_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'sum_prior_chosen_features', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_BLR_confidence_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'BLR_confidence', ['nTrial_rel'])\n",
    "Cryptic_mean_BLR_confidence_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'BLR_confidence', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_signed_confidence_deviation_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'signed_confidence_deviation', ['nTrial_rel'])\n",
    "Cryptic_mean_signed_confidence_deviation_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'signed_confidence_deviation', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_signed_prior_deviation_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'signed_prior_deviation', ['nTrial_rel'])\n",
    "Cryptic_mean_signed_prior_deviation_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'signed_prior_deviation', ['nTrial_rel'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a column for feedback type (1 for correct, 0 for incorrect)\n",
    "# CrypticCreatures['feedback'] = CrypticCreatures['chosen_outcome'].apply(lambda x: 1 if x == 'correct' else 0)\n",
    "\n",
    "# # Create lagged columns for feedback and confidence\n",
    "# CrypticCreatures['prev_feedback'] = CrypticCreatures['feedback'].shift(1)\n",
    "# CrypticCreatures['prev_confidence'] = CrypticCreatures['confidence'].shift(1)\n",
    "\n",
    "# # Filter out the first trial as it has no previous feedback\n",
    "# CrypticCreatures = CrypticCreatures.dropna(subset=['prev_feedback', 'prev_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bc81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summary_rel(df, measurevar, groupvar):\n",
    "    \"\"\"\n",
    "    Calculate mean, standard error, and count for a given measure variable.\n",
    "    \"\"\"\n",
    "    summary = df.groupby(groupvar).agg(\n",
    "        mean=(measurevar, 'mean'),\n",
    "        count=(measurevar, 'size'),\n",
    "        std=(measurevar, 'std')\n",
    "    ).reset_index()\n",
    "    summary['stderr'] = summary['std'] / np.sqrt(summary['count'])\n",
    "    return summary\n",
    "\n",
    "# data frames\n",
    "controls_df = CrypticCreatures_controls_relativeShift \n",
    "patients_df = CrypticCreatures_patients_relativeShift\n",
    "\n",
    "# Merge controls dataset based on 'nTrial_rel' and 'id'\n",
    "controls_merged_df = pd.merge(\n",
    "    CrypticCreatures_BayesianLearner_controls_relativeShift,\n",
    "    controls_df,\n",
    "    on=['nTrial_rel', 'id'],  # Merge on both nTrial_rel and id\n",
    "    how='inner'  # Use inner join to ensure matching nTrial_rel and id\n",
    ")\n",
    "\n",
    "# Merge patients dataset based on 'nTrial_rel' and 'id'\n",
    "patients_merged_df = pd.merge(\n",
    "    CrypticCreatures_BayesianLearner_patients_relativeShift,\n",
    "    patients_df,\n",
    "    on=['nTrial_rel', 'id'],  # Merge on both nTrial_rel and id\n",
    "    how='inner'  # Use inner join to ensure matching nTrial_rel and id\n",
    ")\n",
    "# List of measure variables for controls and patients\n",
    "measure_vars = [\n",
    "    'change_in_mean_accuracy', 'change_in_mean_accuracy_abs', \n",
    "    'change_in_mean_accuracy_ed', 'change_in_mean_accuracy_abs_ed', \n",
    "    'change_in_mean_accuracy_id', 'change_in_mean_accuracy_abs_id',\n",
    "    'change_in_mean_confidence', 'change_in_mean_confidence_abs',\n",
    "    'change_in_mean_confidence_ed', 'change_in_mean_confidence_abs_ed',\n",
    "    'change_in_mean_confidence_id', 'change_in_mean_confidence_abs_id',\n",
    "    'signed_confidence_deviation','signed_prior_deviation','mean_confidence','BLR_confidence',\n",
    "    'mean_accuracy'\n",
    "    \n",
    "]\n",
    "\n",
    "# Calculate summaries for controls and patients\n",
    "summary_controls_rel = {var: calculate_summary_rel(controls_merged_df, var, 'nTrial_rel') for var in measure_vars}\n",
    "summary_patients_rel = {var: calculate_summary_rel(patients_merged_df, var, 'nTrial_rel') for var in measure_vars}\n",
    "print(summary_controls_rel['mean_confidence'])\n",
    "print(Cryptic_mean_conf_controls)\n",
    "# \n",
    "summary_controls_accuracy = summary_controls_rel['change_in_mean_accuracy']\n",
    "summary_patients_accuracy = summary_patients_rel['change_in_mean_accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def summarySE(data, measurevar, groupvars, na_rm=True):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a given measure variable.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame\n",
    "    - measurevar: str, the name of the measure variable\n",
    "    - groupvars: list of str, the names of the grouping variables\n",
    "    - na_rm: bool, whether to remove NA values\n",
    "    \n",
    "    Returns:\n",
    "    - summary: DataFrame with summary statistics\n",
    "    \"\"\"\n",
    "    # Remove NA values if na_rm is True\n",
    "    if na_rm:\n",
    "        data = data.dropna(subset=[measurevar])\n",
    "    \n",
    "    # Group by the specified variables and compute the summary statistics\n",
    "    summary = data.groupby(groupvars).agg(\n",
    "        mean=(measurevar, 'mean'),\n",
    "        count=(measurevar, 'size'),\n",
    "        std=(measurevar, 'std')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Compute the standard error\n",
    "    summary['se'] = summary['std'] / np.sqrt(summary['count'])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Assuming CrypticCreatures_controls_relativeShift and CrypticCreatures_patients_relativeShift are defined as DataFrames\n",
    "\n",
    "# Compute summary statistics for controls\n",
    "Cryptic_mean_confidence_controls = summarySE(\n",
    "    CrypticCreatures_controls_relativeShift, \n",
    "    measurevar=\"mean_confidence\", \n",
    "    groupvars=[\"nTrial_rel\"],\n",
    "    na_rm=True\n",
    ")\n",
    "\n",
    "# Compute summary statistics for patients\n",
    "Cryptic_mean_confidence_patients = summarySE(\n",
    "    CrypticCreatures_patients_relativeShift, \n",
    "    measurevar=\"mean_confidence\", \n",
    "    groupvars=[\"nTrial_rel\"],\n",
    "    na_rm=True\n",
    ")\n",
    "\n",
    "# Print the results to compare\n",
    "print(\"Summary statistics for controls:\")\n",
    "print(Cryptic_mean_confidence_controls)\n",
    "\n",
    "print(\"\\nSummary statistics for patients:\")\n",
    "print(Cryptic_mean_confidence_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399edcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, shapiro, levene, mannwhitneyu\n",
    "\n",
    "# Assuming CrypticCreatures is your dataset\n",
    "# CrypticCreatures = pd.read_csv('path_to_CrypticCreatures.csv')\n",
    "\n",
    "# Compute each individual's average accuracy and confidence\n",
    "CrypticCreatures['average_accuracy'] = CrypticCreatures.groupby('id')['chosen_outcome'].transform('mean')\n",
    "CrypticCreatures['average_confidence'] = CrypticCreatures.groupby('id')['confidence'].transform('mean')\n",
    "average_data = CrypticCreatures[['id', 'average_accuracy', 'average_confidence', 'patientstatus']].drop_duplicates()\n",
    "\n",
    "# Separate the groups\n",
    "controls_acc = average_data[average_data['patientstatus'] == 0]['average_accuracy']\n",
    "patients_acc = average_data[average_data['patientstatus'] == 1]['average_accuracy']\n",
    "controls_conf = average_data[average_data['patientstatus'] == 0]['average_confidence']\n",
    "patients_conf = average_data[average_data['patientstatus'] == 1]['average_confidence']\n",
    "\n",
    "# Function to perform tests and return p-value\n",
    "def perform_tests(controls, patients, measure_name):\n",
    "    # Check for normality\n",
    "    shapiro_controls = shapiro(controls)\n",
    "    shapiro_patients = shapiro(patients)\n",
    "    print(f'Shapiro-Wilk Test for Controls {measure_name}: {shapiro_controls}')\n",
    "    print(f'Shapiro-Wilk Test for Patients {measure_name}: {shapiro_patients}')\n",
    "    \n",
    "    # Check for homogeneity of variances\n",
    "    levene_test = levene(controls, patients)\n",
    "    print(f'Levene\\'s Test for {measure_name}: {levene_test}')\n",
    "    \n",
    "    # Perform t-test if assumptions are met, otherwise use Mann-Whitney U test\n",
    "    if shapiro_controls.pvalue > 0.05 and shapiro_patients.pvalue > 0.05 and levene_test.pvalue > 0.05:\n",
    "        t_stat, p_value = ttest_ind(controls, patients)\n",
    "        print(f'T-test for {measure_name}: T-statistic = {t_stat}, P-value = {p_value}')\n",
    "    else:\n",
    "        u_stat, p_value = mannwhitneyu(controls, patients)\n",
    "        print(f'Mann-Whitney U Test for {measure_name}: U-statistic = {u_stat}, P-value = {p_value}')\n",
    "    return p_value\n",
    "\n",
    "# Perform tests and get p-values\n",
    "p_value_acc = perform_tests(controls_acc, patients_acc, 'Accuracy')\n",
    "p_value_conf = perform_tests(controls_conf, patients_conf, 'Confidence')\n",
    "\n",
    "# Function to draw significance bracket\n",
    "def draw_significance_bracket(ax, x1, x2, y, text, height_percent=0.02):\n",
    "    height = y * height_percent\n",
    "    ax.plot([x1, x1, x2, x2], [y, y + height, y + height, y], lw=1.5, color='black')\n",
    "    ax.plot([x1, x1], [y, y - height], lw=1.5, color='black')  # Left vertical line\n",
    "    ax.plot([x2, x2], [y, y - height], lw=1.5, color='black')  # Right vertical line\n",
    "    ax.text((x1 + x2) * 0.5, y + height * 1.5, text, ha='center', va='bottom', color='black', fontsize=20)\n",
    "\n",
    "# Determine significance text based on p-value\n",
    "def get_significance_text(p_value):\n",
    "    if p_value < 0.001:\n",
    "        return '***'\n",
    "    elif p_value < 0.01:\n",
    "        return '**'\n",
    "    elif p_value < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'n.s.'\n",
    "\n",
    "# Plotting function for accuracy\n",
    "def plot_accuracy_boxplot(average_data, p_value_acc):\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    ax = sns.boxplot(x='patientstatus', y='average_accuracy', data=average_data, palette={0: '#4F67B1' , 1: '#B44B28'})\n",
    "    sns.swarmplot(x='patientstatus', y='average_accuracy', data=average_data, color='black', dodge=True, ax=ax, marker='o', size=5)\n",
    "    \n",
    "    # Add significance indication for accuracy\n",
    "    max_acc = max(average_data['average_accuracy'])\n",
    "    ylim = max_acc + 0.1\n",
    "    significance_text = get_significance_text(p_value_acc)\n",
    "    draw_significance_bracket(ax, 0, 1, ylim - 0.07, significance_text)\n",
    "    \n",
    "    ax.set_ylim(0.4, ylim)\n",
    "    ax.set_xticklabels(['Controls', 'Patients'])\n",
    "    ax.set_yticks(np.arange(0.5, 0.9, 0.1))\n",
    "    ax.set_title('Average Accuracy Comparison')\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_ylabel('Average Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting function for confidence\n",
    "def plot_confidence_boxplot(average_data, p_value_conf):\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    ax = sns.boxplot(x='patientstatus', y='average_confidence', data=average_data, palette={0: '#4F67B1' , 1: '#B44B28'})\n",
    "    sns.swarmplot(x='patientstatus', y='average_confidence', data=average_data, color='black', dodge=True, ax=ax, marker='o', size=5)\n",
    "    \n",
    "    # Add significance indication for confidence\n",
    "    max_conf = max(average_data['average_confidence'])\n",
    "    ylim = max_conf + 10\n",
    "    significance_text = get_significance_text(p_value_conf)\n",
    "    draw_significance_bracket(ax, 0, 1, ylim - 7, significance_text)\n",
    "    \n",
    "    ax.set_ylim(0, ylim)\n",
    "    ax.set_xticklabels(['Controls', 'Patients'])\n",
    "    ax.set_yticks(np.arange(0, 121, 20))\n",
    "    ax.set_title('Average Confidence Comparison')\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_ylabel('Average Confidence')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the boxplots separately\n",
    "plot_accuracy_boxplot(average_data, p_value_acc)\n",
    "plot_confidence_boxplot(average_data, p_value_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05235f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_var(summary_controls, summary_patients, measurevar, title, colors):\n",
    "    \"\"\"\n",
    "    Plot the change in mean var across shifts for controls and patients.\n",
    "    \"\"\"\n",
    "    # Add group labels\n",
    "    summary_controls['patientstatus'] = 0\n",
    "    summary_patients['patientstatus'] = 1\n",
    "    \n",
    "    # Combine datasets\n",
    "    df_combined = pd.concat([summary_controls, summary_patients], ignore_index=True)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for group in [0, 1]:\n",
    "        subset = df_combined[df_combined['patientstatus'] == group]\n",
    "        \n",
    "        # Plot line and scatter points\n",
    "        sns.lineplot(data=subset, x='nTrial_rel', y='mean', color=colors[group])\n",
    "        sns.scatterplot(data=subset, x='nTrial_rel', y='mean', color=colors[group], edgecolor=colors[group], s=100, label='Controls' if group == 0 else 'Patients')\n",
    "\n",
    "        # Add error bars\n",
    "        plt.errorbar(subset['nTrial_rel'], subset['mean'], yerr=subset['stderr'], fmt='o', color=colors[group], capsize=5)\n",
    "    \n",
    "    #plt.axhline(0, color='black', linewidth=1.2, linestyle='--')\n",
    "    plt.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Trial (0=Shifts)')\n",
    "    plt.ylabel('Mean Accuracy')\n",
    "    plt.legend(title='Group')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Optionally, update the global rcParams to set the minimum font size:\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# ============\n",
    "# (1) Adapted Plot Functions that accept an axis\n",
    "# ============\n",
    "def draw_significance_bracket(ax, x1, x2, y, text, height_percent=0.01):\n",
    "    height = y * height_percent\n",
    "    ax.plot([x1, x1, x2, x2], [y, y + height, y + height, y], lw=1.5, color='black')\n",
    "    ax.plot([x1, x1], [y, y - height], lw=1.5, color='black')  # Left vertical line\n",
    "    ax.plot([x2, x2], [y, y - height], lw=1.5, color='black')  # Right vertical line\n",
    "    # Significance text is set to 16 (min)\n",
    "    ax.text((x1 + x2) * 0.5, y + height * 1.5, text, ha='center', va='bottom', \n",
    "            color='black', fontsize=16)\n",
    "\n",
    "def get_significance_text(p_value):\n",
    "    if p_value < 0.001:\n",
    "        return '***'\n",
    "    elif p_value < 0.01:\n",
    "        return '**'\n",
    "    elif p_value < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'n.s.'\n",
    "    \n",
    "def plot_accuracy_boxplot_ax(ax, average_data, p_value_acc):\n",
    "    # Boxplot and swarmplot:\n",
    "    sns.boxplot(x='patientstatus', y='average_accuracy', data=average_data, \n",
    "                palette={0: '#4F67B1' , 1: '#B44B28'}, ax=ax, width=0.3, linewidth=1.5)\n",
    "    sns.stripplot(x='patientstatus', y='average_accuracy', data=average_data, \n",
    "                  color='#222328', jitter=True, ax=ax, marker='o', size=4)\n",
    "    # Determine y-limit and add significance bracket:\n",
    "    max_acc = average_data['average_accuracy'].max()\n",
    "    ylim = max_acc + 0.1\n",
    "    significance_text = get_significance_text(p_value_acc)\n",
    "    draw_significance_bracket(ax, 0, 1, ylim - 0.07, significance_text)\n",
    "    \n",
    "    # Format the axis:\n",
    "    ax.set_ylim(0.4, ylim)\n",
    "    ax.set_xticks([0.5])\n",
    "    ax.set_xticklabels(['Overall'])\n",
    "    ax.set_yticks(np.arange(0.5, 0.9, 0.1))\n",
    "    ax.set_xlabel(' ')\n",
    "    # Increase y-label font size to 18:\n",
    "    ax.set_ylabel('Average Accuracy', fontweight='bold', fontsize=16)\n",
    "    # Set tick label size to at least 16:\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "    \n",
    "    sns.despine()\n",
    "\n",
    "def plot_confidence_boxplot_ax(ax, average_data, p_value_conf):\n",
    "    sns.boxplot(x='patientstatus', y='average_confidence', data=average_data, \n",
    "                palette={0: '#4F67B1' , 1: '#B44B28'}, ax=ax, width=0.3,linewidth=1.5)\n",
    "    sns.stripplot(x='patientstatus', y='average_confidence', data=average_data, \n",
    "                  color='#222328', jitter=True, ax=ax, marker='o', size=4)\n",
    "    \n",
    "    max_conf = average_data['average_confidence'].max()\n",
    "    ylim = max_conf + 10\n",
    "    significance_text = get_significance_text(p_value_conf)\n",
    "    draw_significance_bracket(ax, 0, 1, ylim - 7, significance_text)\n",
    "    \n",
    "    ax.set_ylim(0, ylim)\n",
    "    ax.set_xticks([0.5])\n",
    "    ax.set_xticklabels(['Overall'])\n",
    "    ax.set_yticks(np.arange(0, 121, 20))\n",
    "    ax.set_xlabel(' ')\n",
    "    ax.set_ylabel('Average Confidence', fontweight='bold', fontsize=16)\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "    \n",
    "    sns.despine()\n",
    "\n",
    "def plot_mean_var_ax(ax, summary_controls, summary_patients, measurevar, ylabel, xlabel, ylim, yticks, colors):\n",
    "    # Convert inputs to DataFrames and add a group label:\n",
    "    summary_controls = pd.DataFrame(summary_controls)\n",
    "    summary_patients = pd.DataFrame(summary_patients)\n",
    "    summary_controls['patientstatus'] = 'Controls'\n",
    "    summary_patients['patientstatus'] = 'Patients'\n",
    "    \n",
    "    df_combined = pd.concat([summary_controls, summary_patients], ignore_index=True)\n",
    "    ax.axvline(0, color='black', linewidth=1, linestyle='--')\n",
    "    # Plot lines, scatter points, and error bars:\n",
    "    for group_key, color in colors.items():\n",
    "        label = 'Controls' if group_key == 0 else 'Patients'\n",
    "        subset = df_combined[df_combined['patientstatus'] == label]\n",
    "        \n",
    "        sns.lineplot(data=subset, x='nTrial_rel', y='mean', color=color, ax=ax)\n",
    "        sns.scatterplot(data=subset, x='nTrial_rel', y='mean', color=color, \n",
    "                        edgecolor='white', s=100, label=label, ax=ax)\n",
    "        ax.errorbar(subset['nTrial_rel'], subset['mean'], yerr=subset['stderr'], \n",
    "                    fmt='none', color=color, capsize=5)\n",
    "    \n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_xticks(np.arange(-5, 6, 1))\n",
    "    \n",
    "    # Increase x- and y-label font sizes to 18:\n",
    "    ax.set_xlabel(' ', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel(ylabel, fontsize=16)\n",
    "    ax.set_xlabel(xlabel, fontsize=16)\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "    # Update legend font sizes:\n",
    "    if measurevar == 'mean_accuracy':\n",
    "        ax.legend(title='Group', fontsize=14, title_fontsize=14, loc='lower left')\n",
    "    else:\n",
    "        legend = ax.get_legend()\n",
    "        if legend is not None:\n",
    "            legend.remove()\n",
    "    sns.despine()\n",
    "\n",
    "# ============\n",
    "# (2) Create the Combined Figure\n",
    "# ============\n",
    "# (Note: average_data, p_value_acc, p_value_conf, summary_controls_rel, \n",
    "# and summary_patients_rel should be defined in your workspace.)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    2, 2, \n",
    "    figsize=(10, 10), \n",
    "    sharey='row', \n",
    "    gridspec_kw={'width_ratios': [1, 2], 'wspace': 0}\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(wspace=0)\n",
    "\n",
    "# Plot accuracy boxplot and mean variance plot:\n",
    "plot_accuracy_boxplot_ax(axes[0, 0], average_data, p_value_acc)\n",
    "plot_mean_var_ax(axes[0, 1], \n",
    "                 summary_controls_rel['mean_accuracy'], \n",
    "                 summary_patients_rel['mean_accuracy'], \n",
    "                 'mean_accuracy', 'Mean Accuracy',' ',\n",
    "                 (0, 1), np.arange(0, 1.1, 0.2), \n",
    "                 colors={0: '#4F67B1' , 1: '#B44B28'})\n",
    "\n",
    "# Add 'A' label to the upper left corner of the first row of plots\n",
    "axes[0, 0].text(-0.1, 1.1, 'A', transform=axes[0, 0].transAxes, \n",
    "                fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "\n",
    "# Plot confidence boxplot and mean variance plot:\n",
    "plot_confidence_boxplot_ax(axes[1, 0], average_data, p_value_conf)\n",
    "plot_mean_var_ax(axes[1, 1], \n",
    "                 summary_controls_rel['mean_confidence'], \n",
    "                 summary_patients_rel['mean_confidence'], \n",
    "                 'mean_confidence', 'Mean Confidence', 'Trial (0=Shifts)',\n",
    "                 (20, 115), np.arange(20, 110, 20), \n",
    "                 colors={0: '#4F67B1' , 1: '#B44B28'})\n",
    "\n",
    "# Add 'B' label to the upper left corner of the second row of plots\n",
    "axes[1, 0].text(-0.1, 1.1, 'B', transform=axes[1, 0].transAxes, \n",
    "                fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Create the subfolder if it doesn't exist\n",
    "if not os.path.exists('../figures'):\n",
    "    os.makedirs('../figures')\n",
    "\n",
    "# Save the figure into the subfolder\n",
    "plt.savefig('../figures/Fig2_AccConf_Average.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54f9f4",
   "metadata": {},
   "source": [
    "Below are plots to look at changes in variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b244e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind, shapiro, levene, mannwhitneyu\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Update global font size\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# --- Helper Functions for Significance Annotation ---\n",
    "def draw_significance_bracket(ax, x1, x2, y, text, height_percent=0.1):\n",
    "    height = y * height_percent\n",
    "    # Draw bracket\n",
    "    ax.plot([x1, x1, x2, x2], [y, y + height, y + height, y], lw=1.5, color='black')\n",
    "    # Place significance text\n",
    "    ax.text((x1 + x2) * 0.5, y + height * 1.5, text, ha='center', va='bottom', \n",
    "            color='black', fontsize=16)\n",
    "\n",
    "def get_significance_text(p_value):\n",
    "    if p_value < 0.001:\n",
    "        return '***'\n",
    "    elif p_value < 0.01:\n",
    "        return '**'\n",
    "    elif p_value < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'n.s.'\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Sort and create an overall trial variable (\"trial_all\")\n",
    "# -------------------------------\n",
    "sort_cols = ['id']\n",
    "for col in ['task_id', 'run', 'trial']:\n",
    "    if col in CrypticCreatures.columns:\n",
    "        sort_cols.append(col)\n",
    "CrypticCreatures = CrypticCreatures.sort_values(by=sort_cols)\n",
    "CrypticCreatures['trial_all'] = CrypticCreatures.groupby('id').cumcount() + 1\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Compute previous trial outcome and change in confidence\n",
    "# -------------------------------\n",
    "CrypticCreatures['prev_outcome'] = CrypticCreatures.groupby('id')['chosen_outcome'].shift(1)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Mark trials based on the previous trial's outcome\n",
    "# -------------------------------\n",
    "CrypticCreatures['trial_type'] = np.where(\n",
    "    CrypticCreatures['prev_outcome'] == 1, 'after_correct',\n",
    "    np.where(CrypticCreatures['prev_outcome'] == 0, 'after_incorrect', np.nan)\n",
    ")\n",
    "df_marked = CrypticCreatures.dropna(subset=['trial_type'])\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Aggregate the change scores per participant for each trial type\n",
    "# -------------------------------\n",
    "avg_change = (\n",
    "    df_marked.groupby(['id', 'trial_type'])['confidence_change']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "avg_change_pivot = avg_change.pivot(index='id', columns='trial_type', values='confidence_change').reset_index()\n",
    "avg_change_pivot = avg_change_pivot.drop('nan', axis=1)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Merge patient status information (assuming one value per participant)\n",
    "# -------------------------------\n",
    "patient_info = CrypticCreatures.groupby('id')['patientstatus'].first().reset_index()\n",
    "avg_change_pivot = pd.merge(avg_change_pivot, patient_info, on='id')\n",
    "print(avg_change_pivot.head())\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Group comparisons: tests for each change type (with assumption checks)\n",
    "# -------------------------------\n",
    "controls = avg_change_pivot[avg_change_pivot['patientstatus'] == 0]\n",
    "patients = avg_change_pivot[avg_change_pivot['patientstatus'] == 1]\n",
    "\n",
    "# Initialize significance p-value storage\n",
    "significance_correct = None\n",
    "significance_incorrect = None\n",
    "\n",
    "# --- After Correct Trials ---\n",
    "after_correct_controls = controls['after_correct'].dropna()\n",
    "after_correct_patients = patients['after_correct'].dropna()\n",
    "\n",
    "if len(after_correct_controls) > 0 and len(after_correct_patients) > 0:\n",
    "    # Check normality\n",
    "    shapiro_controls = shapiro(after_correct_controls)\n",
    "    shapiro_patients = shapiro(after_correct_patients)\n",
    "    # Check variance homogeneity\n",
    "    levene_test = levene(after_correct_controls, after_correct_patients)\n",
    "\n",
    "    print(\"After Correct Trials Assumptions:\")\n",
    "    print(f\"  Shapiro p-values: Controls={shapiro_controls.pvalue:.3f}, Patients={shapiro_patients.pvalue:.3f}\")\n",
    "    print(f\"  Levene p-value: {levene_test.pvalue:.3f}\")\n",
    "\n",
    "    if (shapiro_controls.pvalue > 0.05 and shapiro_patients.pvalue > 0.05) and (levene_test.pvalue > 0.05):\n",
    "        # Perform t-test\n",
    "        tstat_correct, pval_correct = ttest_ind(after_correct_controls, after_correct_patients)\n",
    "        print(\"After Correct Trials: t-test used\")\n",
    "        print(\"t = {:.3f}, p = {:.3f}\".format(tstat_correct, pval_correct))\n",
    "        significance_correct = pval_correct\n",
    "    else:\n",
    "        # Use nonparametric test\n",
    "        u_stat, pval_correct = mannwhitneyu(after_correct_controls, after_correct_patients, alternative='two-sided')\n",
    "        print(\"After Correct Trials: Mann-Whitney U test used\")\n",
    "        print(\"U = {:.3f}, p = {:.3f}\".format(u_stat, pval_correct))\n",
    "        significance_correct = pval_correct\n",
    "else:\n",
    "    print(\"Insufficient data for testing after correct trials.\")\n",
    "\n",
    "# --- After Incorrect Trials ---\n",
    "after_incorrect_controls = controls['after_incorrect'].dropna()\n",
    "after_incorrect_patients = patients['after_incorrect'].dropna()\n",
    "\n",
    "if len(after_incorrect_controls) > 0 and len(after_incorrect_patients) > 0:\n",
    "    # Check normality\n",
    "    shapiro_controls = shapiro(after_incorrect_controls)\n",
    "    shapiro_patients = shapiro(after_incorrect_patients)\n",
    "    # Check variance homogeneity\n",
    "    levene_test = levene(after_incorrect_controls, after_incorrect_patients)\n",
    "\n",
    "    print(\"\\nAfter Incorrect Trials Assumptions:\")\n",
    "    print(f\"  Shapiro p-values: Controls={shapiro_controls.pvalue:.3f}, Patients={shapiro_patients.pvalue:.3f}\")\n",
    "    print(f\"  Levene p-value: {levene_test.pvalue:.3f}\")\n",
    "\n",
    "    if (shapiro_controls.pvalue > 0.05 and shapiro_patients.pvalue > 0.05) and (levene_test.pvalue > 0.05):\n",
    "        # Perform t-test\n",
    "        tstat_incorrect, pval_incorrect = ttest_ind(after_incorrect_controls, after_incorrect_patients)\n",
    "        print(\"After Incorrect Trials: t-test used\")\n",
    "        print(\"t = {:.3f}, p = {:.3f}\".format(tstat_incorrect, pval_incorrect))\n",
    "        significance_incorrect = pval_incorrect\n",
    "    else:\n",
    "        # Use nonparametric test\n",
    "        u_stat, pval_incorrect = mannwhitneyu(after_incorrect_controls, after_incorrect_patients, alternative='two-sided')\n",
    "        print(\"After Incorrect Trials: Mann-Whitney U test used\")\n",
    "        print(\"U = {:.3f}, p = {:.3f}\".format(u_stat, pval_incorrect))\n",
    "        significance_incorrect = pval_incorrect\n",
    "else:\n",
    "    print(\"Insufficient data for testing after incorrect trials.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Plot boxplots of average change scores by patient status\n",
    "# -------------------------------\n",
    "sns.set_style(\"white\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "# Left plot: After Correct Trials\n",
    "sns.boxplot(\n",
    "    data=avg_change_pivot,\n",
    "    x='patientstatus', y='after_correct',\n",
    "    palette=[\"#4F67B1\", \"#B44B28\"],\n",
    "    ax=axes[0],\n",
    "    showfliers=False\n",
    ")\n",
    "sns.stripplot(\n",
    "    data=avg_change_pivot,\n",
    "    x='patientstatus', y='after_correct',\n",
    "    color=\"#222328\",\n",
    "    size=5,\n",
    "    jitter=True,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].axhline(y=0, linestyle=\"--\", color=\"#333333\", linewidth=1)\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[0].set_ylabel(\"Average Confidence Change\\nAfter Correct Trials\", fontsize=16, fontweight=\"bold\")\n",
    "axes[0].set_xticklabels([\" \", \" \"], fontsize=14)\n",
    "axes[0].tick_params(axis='y', labelsize=14)\n",
    "axes[0].tick_params(axis='x', bottom=None)\n",
    "axes[0].tick_params(axis='y', left=True)\n",
    "sns.despine(ax=axes[0])\n",
    "# Place label A above the plot\n",
    "axes[0].text(0.02, 1.03, \"A\", transform=axes[0].transAxes,\n",
    "             fontsize=20, fontweight=\"bold\", va=\"bottom\", ha=\"left\")\n",
    "\n",
    "# Right plot: After Incorrect Trials\n",
    "sns.boxplot(\n",
    "    data=avg_change_pivot,\n",
    "    x='patientstatus', y='after_incorrect',\n",
    "    palette=[\"#4F67B1\", \"#B44B28\"],\n",
    "    ax=axes[1],\n",
    "    showfliers=False\n",
    ")\n",
    "sns.stripplot(\n",
    "    data=avg_change_pivot,\n",
    "    x='patientstatus', y='after_incorrect',\n",
    "    color=\"#222328\",\n",
    "    size=5,\n",
    "    jitter=True,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].axhline(y=0, linestyle=\"--\", color=\"#333333\", linewidth=1)\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[1].set_ylabel(\"Average Confidence Change\\nAfter Incorrect Trials\", fontsize=16, fontweight=\"bold\")\n",
    "axes[1].set_xticklabels([\" \", \" \"], fontsize=14)\n",
    "axes[1].tick_params(axis='y', labelsize=14)\n",
    "axes[1].tick_params(axis='x', bottom=None)\n",
    "axes[1].tick_params(axis='y', left=True)\n",
    "sns.despine(ax=axes[1])\n",
    "# Place label B above the plot\n",
    "axes[1].text(0.02, 1.03, \"B\", transform=axes[1].transAxes,\n",
    "             fontsize=20, fontweight=\"bold\", va=\"bottom\", ha=\"left\")\n",
    "# Create legend handles\n",
    "control_patch = mlines.Line2D([], [], color=\"#4F67B1\", marker=\"s\", linestyle=\"None\", markersize=10, label=\"Control\")\n",
    "patient_patch = mlines.Line2D([], [], color=\"#B44B28\", marker=\"s\", linestyle=\"None\", markersize=10, label=\"Patient\")\n",
    "axes[0].legend(handles=[control_patch, patient_patch], title='Group', fontsize=14, title_fontsize=14, loc='lower left')\n",
    "# Ensure both plots share the same y-limits so the zero-line is aligned\n",
    "common_ylim = [min(axes[0].get_ylim()[0], axes[1].get_ylim()[0]),\n",
    "               max(axes[0].get_ylim()[1], axes[1].get_ylim()[1])+2]\n",
    "axes[0].set_ylim(common_ylim)\n",
    "axes[1].set_ylim(common_ylim)\n",
    "\n",
    "# --- Add significance brackets using the helper functions ---\n",
    "if significance_correct is not None:\n",
    "    sig_text = get_significance_text(significance_correct)\n",
    "    # Increase padding by adding a constant rather than a multiplier\n",
    "    y_max = avg_change_pivot['after_correct'].max() + 0.15\n",
    "    # Increase the bracket height using a higher height_percent value\n",
    "    draw_significance_bracket(axes[0], 0, 1, y_max, sig_text)\n",
    "    \n",
    "if significance_incorrect is not None:\n",
    "    sig_text = get_significance_text(significance_incorrect)\n",
    "    # Use the maximum y-value from the after_incorrect data with padding\n",
    "    y_max = avg_change_pivot['after_incorrect'].max() * 1.1\n",
    "    draw_significance_bracket(axes[1], 0, 1, y_max, sig_text)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adfbf1c",
   "metadata": {},
   "source": [
    "Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import statsmodels.formula.api as smf\n",
    "\n",
    "# # Create some example data\n",
    "# df = pd.DataFrame({\n",
    "#     'y': [1, 2, 3, 4, 5],\n",
    "#     'x': [2, 1, 4, 3, 5]\n",
    "# })\n",
    "\n",
    "# # Fit a simple linear model\n",
    "# model = smf.ols(formula='y ~ x', data=df).fit()\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Define a helper function to compute z-scores within a group.\n",
    "def zscore_within(x):\n",
    "    m = x.mean()\n",
    "    s = x.std(ddof=0)\n",
    "    return (x - m) / s if s != 0 else x - m\n",
    "\n",
    "# ===========================================\n",
    "# 1. Create the .sc variables\n",
    "# ===========================================\n",
    "# Z-score the trial-level 'confidence' variable within each id.\n",
    "CrypticCreatures['confidence.sc'] = CrypticCreatures.groupby('id')['confidence'].transform(zscore_within)\n",
    "\n",
    "# For participant-level variables, perform global (across-person) z-scoring.\n",
    "if 'age' in CrypticCreatures.columns:\n",
    "    CrypticCreatures['age.sc'] = (CrypticCreatures['age'] - CrypticCreatures['age'].mean()) / CrypticCreatures['age'].std(ddof=0)\n",
    "if 'icar_totalscore' in CrypticCreatures.columns:\n",
    "    CrypticCreatures['total_iq.sc'] = (CrypticCreatures['icar_totalscore'] - CrypticCreatures['icar_totalscore'].mean()) / CrypticCreatures['icar_totalscore'].std(ddof=0)\n",
    "\n",
    "# ===========================================\n",
    "# 2. Ensure that categorical variables are of type 'category'\n",
    "# ===========================================\n",
    "CrypticCreatures['patientstatus'] = CrypticCreatures['patientstatus'].astype('category')\n",
    "CrypticCreatures['chosen_outcome_shift'] = CrypticCreatures['chosen_outcome_shift'].astype('category')\n",
    "CrypticCreatures['task_id'] = CrypticCreatures['task_id'].astype('category')\n",
    "\n",
    "# ===========================================\n",
    "# 3. Define and fit the mixed-effects regression model\n",
    "# ===========================================\n",
    "# # Wrap variable names with dots using Q()\n",
    "# formula = (\"Q('confidence.sc') ~ C(patientstatus)*C(chosen_outcome_shift) + Q('age.sc') + \"\n",
    "#            \"gender + Q('total_iq.sc')\")\n",
    "\n",
    "# # Fit the model with random effects:\n",
    "# #   - Random intercept and slope for chosen_outcome_shift within each id.\n",
    "# #   - Random intercept for task_id specified in vc_formula.\n",
    "# model = smf.mixedlm(formula,\n",
    "#                     data=CrypticCreatures,\n",
    "#                     groups=CrypticCreatures[\"id\"],\n",
    "#                     re_formula=\"1 + C(chosen_outcome_shift)\",\n",
    "#                     vc_formula={\"task_id\": \"0 + C(task_id)\"})\n",
    "\n",
    "# result = model.fit(method='lbfgs', maxiter=100000)\n",
    "# print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( CrypticCreatures['age.sc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f9868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymer4.models import Lmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Replace the string \"nan\" with an actual NaN\n",
    "CrypticCreatures['chosen_outcome_shift'] = CrypticCreatures['chosen_outcome_shift'].replace(\"nan\", np.nan)\n",
    "\n",
    "# Drop rows where chosen_outcome_shift is NaN\n",
    "CrypticCreatures = CrypticCreatures.dropna(subset=['chosen_outcome_shift'])\n",
    "\n",
    "# Convert columns to strings and then to categorical.\n",
    "# This ensures that the factor levels are strings, which is what R expects.\n",
    "CrypticCreatures['patientstatus'] = CrypticCreatures['patientstatus'].astype(str).astype('category')\n",
    "CrypticCreatures['chosen_outcome_shift'] = CrypticCreatures['chosen_outcome_shift'].astype(str).astype('category')\n",
    "CrypticCreatures['task_id'] = CrypticCreatures['task_id'].astype(str).astype('category')\n",
    "if 'gender' in CrypticCreatures.columns:\n",
    "    CrypticCreatures['gender'] = CrypticCreatures['gender'].astype(str).astype('category')\n",
    "\n",
    "# Remove any unused categories.\n",
    "CrypticCreatures['patientstatus'] = CrypticCreatures['patientstatus'].cat.remove_unused_categories()\n",
    "CrypticCreatures['chosen_outcome_shift'] = CrypticCreatures['chosen_outcome_shift'].cat.remove_unused_categories()\n",
    "CrypticCreatures['task_id'] = CrypticCreatures['task_id'].cat.remove_unused_categories()\n",
    "if 'gender' in CrypticCreatures.columns:\n",
    "    CrypticCreatures['gender'] = CrypticCreatures['gender'].cat.remove_unused_categories()\n",
    "\n",
    "# Print out the levels to verify.\n",
    "print(\"patientstatus levels:\", list(CrypticCreatures['patientstatus'].cat.categories))\n",
    "print(\"chosen_outcome_shift levels:\", list(CrypticCreatures['chosen_outcome_shift'].cat.categories))\n",
    "print(\"task_id levels:\", list(CrypticCreatures['task_id'].cat.categories))\n",
    "if 'gender' in CrypticCreatures.columns:\n",
    "    print(\"gender levels:\", list(CrypticCreatures['gender'].cat.categories))\n",
    "\n",
    "# Create the factors dictionary using the string levels.\n",
    "factors = {\n",
    "    'patientstatus': list(CrypticCreatures['patientstatus'].cat.categories),\n",
    "    'chosen_outcome_shift': list(CrypticCreatures['chosen_outcome_shift'].cat.categories),\n",
    "    'task_id': list(CrypticCreatures['task_id'].cat.categories)\n",
    "}\n",
    "if 'gender' in CrypticCreatures.columns:\n",
    "    factors['gender'] = list(CrypticCreatures['gender'].cat.categories)\n",
    "\n",
    "print(\"Factors dictionary:\", factors)\n",
    "\n",
    "# Specify and fit the model using pymer4.\n",
    "model = Lmer(\"confidence ~ patientstatus * chosen_outcome_shift + age.sc + gender + total_iq.sc + (1+chosen_outcome_shift|id) + (1|task_id)\",\n",
    "             data=CrypticCreatures)\n",
    "result = model.fit(factors=factors)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0519871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Copy the coefficients DataFrame from your fitted model.\n",
    "coefs = model.coefs.copy()\n",
    "\n",
    "# --- Step 1: Remove intercept and demographic predictors ---\n",
    "for intercept in ['(Intercept)', 'Intercept']:\n",
    "    if intercept in coefs.index:\n",
    "        coefs = coefs.drop(intercept)\n",
    "\n",
    "# Remove demographics: assuming these are age.sc, gender, and total_iq.sc.\n",
    "demographics = [\"age.sc\", \"gender\", \"total_iq.sc\"]\n",
    "pattern = '|'.join(demographics)\n",
    "coefs = coefs[~coefs.index.str.contains(pattern)]\n",
    "\n",
    "# --- Step 2: Create pretty labels with new variable names ---\n",
    "def pretty_label(coef_name):\n",
    "    \"\"\"\n",
    "    Converts a coefficient name into a human-friendly label.\n",
    "    \n",
    "    - For patientstatus, returns \"Group: <level>\"\n",
    "    - For chosen_outcome_shift, returns \"Preceding Accuracy: <level>\"\n",
    "    - For an interaction, returns \"Group: <level> × Preceding Accuracy: <level>\"\n",
    "    \"\"\"\n",
    "    if \":\" in coef_name:\n",
    "        # Interaction term (e.g., \"patientstatus[T.X]:chosen_outcome_shift[T.Y]\")\n",
    "        group_match = re.search(r'patientstatus\\[T\\.(.*?)\\]', coef_name)\n",
    "        acc_match = re.search(r'chosen_outcome_shift\\[T\\.(.*?)\\]', coef_name)\n",
    "        if group_match and acc_match:\n",
    "            return f\"Group: {group_match.group(1)} × Preceding Accuracy: {acc_match.group(1)}\"\n",
    "    elif \"patientstatus\" in coef_name:\n",
    "        group_match = re.search(r'patientstatus\\[T\\.(.*?)\\]', coef_name)\n",
    "        if group_match:\n",
    "            return f\"Group: {group_match.group(1)}\"\n",
    "    elif \"chosen_outcome_shift\" in coef_name:\n",
    "        acc_match = re.search(r'chosen_outcome_shift\\[T\\.(.*?)\\]', coef_name)\n",
    "        if acc_match:\n",
    "            return f\"Preceding Accuracy: {acc_match.group(1)}\"\n",
    "    return coef_name\n",
    "\n",
    "# Generate pretty labels for each coefficient.\n",
    "pretty_labels = [pretty_label(name) for name in coefs.index]\n",
    "coefs['Pretty'] = pretty_labels\n",
    "\n",
    "# --- Step 3: Remove duplicate interaction effects, if any ---\n",
    "coefs = coefs[~coefs['Pretty'].duplicated()]\n",
    "\n",
    "# --- Step 4: Define a function to convert p-values to significance symbols ---\n",
    "def significance_symbol(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# --- Step 5: Plot with predictors on the x-axis ---\n",
    "x_positions = range(len(coefs))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot estimates with vertical error bars.\n",
    "ax.errorbar(x_positions, coefs['Estimate'], yerr=coefs['SE'],\n",
    "            fmt='o', color=\"#9D8B54\", ecolor=\"#9D8B54\", elinewidth=2, capsize=4)\n",
    "\n",
    "# Draw a horizontal zero line in grey.\n",
    "ax.axhline(0, linestyle='--', color='grey', linewidth=1)\n",
    "\n",
    "# Set x-axis labels using the pretty labels.\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(coefs['Pretty'], rotation=45, ha='right')\n",
    "ax.set_ylabel('Effect Size')\n",
    "\n",
    "# Remove the top and right spines.\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# --- Step 6: Annotate each point with significance symbols ---\n",
    "# (Assumes that the p-values are stored in a column called 'P-val'.)\n",
    "for x, (_, row) in zip(x_positions, coefs.iterrows()):\n",
    "    # Calculate significance symbol based on the p-value.\n",
    "    # Adjust the column name below if your p-value column is named differently.\n",
    "    p_val = row['P-val']  \n",
    "    sig = significance_symbol(p_val)\n",
    "    # Annotate above the error bar; adjust offset as needed.\n",
    "    y = row['Estimate'] + row['SE'] + 0.5\n",
    "    ax.text(x, y, sig, ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.testing import assert_frame_equal\n",
    "# # Import the dataset\n",
    "# testCrypticCreatures = pd.read_csv(\"testCrypticCreatures2.csv\")\n",
    "\n",
    "# # Rename the existing CrypticCreatures to df1\n",
    "# df1 = CrypticCreatures.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "\n",
    "# # Rename the imported dataset to df2\n",
    "# df2 = CrypticCreatures2.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "# try:\n",
    "#     # Use check_exact=False and set tolerances if you expect minor numerical differences.\n",
    "#     assert_frame_equal(df1, df2, check_exact=False, rtol=1e-5, atol=1e-8)\n",
    "#     print(\"The dataframes are equal!\")\n",
    "# except AssertionError as e:\n",
    "#     print(\"Differences found between dataframes:\\n\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import datacompy\n",
    "\n",
    "# # Define join columns\n",
    "# join_cols = ['id', 'task_id', 'run', 'trial']\n",
    "\n",
    "# # Ensure key columns are stripped and converted to numeric\n",
    "# for col in join_cols:\n",
    "#     # Remove leading/trailing spaces (if any)\n",
    "#     df1[col] = df1[col].astype(str).str.strip()\n",
    "#     df2[col] = df2[col].astype(str).str.strip()\n",
    "    \n",
    "#     # Convert to numeric if applicable\n",
    "#     df1[col] = pd.to_numeric(df1[col], errors='coerce')\n",
    "#     df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "\n",
    "# # Sort both DataFrames and reset the index\n",
    "# df1_sorted = df1.sort_values(by=join_cols).reset_index(drop=True)\n",
    "# df2_sorted = df2.sort_values(by=join_cols).reset_index(drop=True)\n",
    "\n",
    "# # Check sorted order\n",
    "# print(\"df1_sorted head:\")\n",
    "# print(df1_sorted[join_cols].head(20))\n",
    "# print(\"df2_sorted head:\")\n",
    "# print(df2_sorted[join_cols].head(20))\n",
    "\n",
    "# # Compare using datacompy\n",
    "# compare = datacompy.Compare(\n",
    "#     df1_sorted, df2_sorted,\n",
    "#     join_columns=join_cols,\n",
    "#     abs_tol=1e-5,\n",
    "#     rel_tol=1e-5,\n",
    "#     df1_name='df1_sorted',\n",
    "#     df2_name='df2_sorted'\n",
    "# )\n",
    "# print(compare.report())\n",
    "# # Get the unique id values from each DataFrame\n",
    "# ids_df1 = set(df1['id'].unique())\n",
    "# ids_df2 = set(df2['id'].unique())\n",
    "\n",
    "# # Compare the sets\n",
    "# if ids_df1 == ids_df2:\n",
    "#     print(\"Both DataFrames have the same unique id values.\")\n",
    "# else:\n",
    "#     print(\"The DataFrames have different unique id values.\")\n",
    "#     # Show differences if needed:\n",
    "#     print(\"IDs in df1 but not in df2:\", ids_df1 - ids_df2)\n",
    "#     print(\"IDs in df2 but not in df1:\", ids_df2 - ids_df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96660675",
   "metadata": {},
   "source": [
    "Supplemental Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f113033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_change_in_mean_var(summary_controls, summary_patients, measurevar, title, ydesc, colors,ylims):\n",
    "    \"\"\"\n",
    "    Plot the change in mean var across shifts for controls and patients.\n",
    "    \"\"\"\n",
    "    # Add group labels\n",
    "    summary_controls['patientstatus'] = 0\n",
    "    summary_patients['patientstatus'] = 1\n",
    "    \n",
    "    # Combine datasets\n",
    "    df_combined = pd.concat([summary_controls, summary_patients], ignore_index=True)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for group in [0, 1]:\n",
    "        subset = df_combined[df_combined['patientstatus'] == group]\n",
    "        \n",
    "        # Plot line and scatter points\n",
    "        sns.lineplot(data=subset, x='nTrial_rel', y='mean', color=colors[group])\n",
    "        sns.scatterplot(data=subset, x='nTrial_rel', y='mean', color=colors[group], edgecolor=colors[group], s=100, label='Controls' if group == 0 else 'Patients')\n",
    "\n",
    "        # Add error bars\n",
    "        plt.errorbar(subset['nTrial_rel'], subset['mean'], yerr=subset['stderr'], fmt='o', color=colors[group], capsize=5)\n",
    "    \n",
    "    plt.axhline(0, color='black', linewidth=1.2, linestyle='--')\n",
    "    plt.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    plt.ylim(ylims)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Trial (0=Shifts)')\n",
    "    plt.ylabel(ydesc)\n",
    "    plt.legend(title='Group')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_change_in_mean_var(\n",
    "    summary_controls_rel['change_in_mean_accuracy'], \n",
    "    summary_patients_rel['change_in_mean_accuracy'], \n",
    "    'change_in_mean_accuracy', \n",
    "    ' ', \n",
    "    'Accuracy Delta',\n",
    "    colors = {0: 'olive', 1: 'darkblue'},\n",
    "    ylims = (-0.7, 0.4)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ba776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confidence \n",
    "plot_change_in_mean_var(\n",
    "    summary_controls_rel['change_in_mean_confidence'], \n",
    "    summary_patients_rel['change_in_mean_confidence'], \n",
    "    'change_in_mean_confidence', \n",
    "    ' ',\n",
    "    'Confidence Delta',\n",
    "     colors = {0: '#16463F', 1: '#2D80A7'},\n",
    "     ylims = (-19,6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7ef3e",
   "metadata": {},
   "source": [
    "Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3322b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import statsmodels.formula.api as smf\n",
    "\n",
    "# # Create some example data\n",
    "# df = pd.DataFrame({\n",
    "#     'y': [1, 2, 3, 4, 5],\n",
    "#     'x': [2, 1, 4, 3, 5]\n",
    "# })\n",
    "\n",
    "# # Fit a simple linear model\n",
    "# model = smf.ols(formula='y ~ x', data=df).fit()\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Define a helper function to compute z-scores within a group.\n",
    "def zscore_within(x):\n",
    "    m = x.mean()\n",
    "    s = x.std(ddof=0)\n",
    "    return (x - m) / s if s != 0 else x - m\n",
    "\n",
    "# ===========================================\n",
    "# 1. Create the .sc variables\n",
    "# ===========================================\n",
    "# Z-score the trial-level 'confidence' variable within each id.\n",
    "CrypticCreatures['confidence.sc'] = CrypticCreatures.groupby('id')['confidence'].transform(zscore_within)\n",
    "\n",
    "# For participant-level variables, perform global (across-person) z-scoring.\n",
    "if 'age' in CrypticCreatures.columns:\n",
    "    CrypticCreatures['age.sc'] = (CrypticCreatures['age'] - CrypticCreatures['age'].mean()) / CrypticCreatures['age'].std(ddof=0)\n",
    "if 'icar_totalscore' in CrypticCreatures.columns:\n",
    "    CrypticCreatures['total_iq.sc'] = (CrypticCreatures['icar_totalscore'] - CrypticCreatures['icar_totalscore'].mean()) / CrypticCreatures['icar_totalscore'].std(ddof=0)\n",
    "\n",
    "# ===========================================\n",
    "# 2. Ensure that categorical variables are of type 'category'\n",
    "# ===========================================\n",
    "CrypticCreatures['patientstatus'] = CrypticCreatures['patientstatus'].astype('category')\n",
    "CrypticCreatures['chosen_outcome_shift'] = CrypticCreatures['chosen_outcome_shift'].astype('category')\n",
    "CrypticCreatures['task_id'] = CrypticCreatures['task_id'].astype('category')\n",
    "\n",
    "# ===========================================\n",
    "# 3. Define and fit the mixed-effects regression model\n",
    "# ===========================================\n",
    "# # Wrap variable names with dots using Q()\n",
    "# formula = (\"Q('confidence.sc') ~ C(patientstatus)*C(chosen_outcome_shift) + Q('age.sc') + \"\n",
    "#            \"gender + Q('total_iq.sc')\")\n",
    "\n",
    "# # Fit the model with random effects:\n",
    "# #   - Random intercept and slope for chosen_outcome_shift within each id.\n",
    "# #   - Random intercept for task_id specified in vc_formula.\n",
    "# model = smf.mixedlm(formula,\n",
    "#                     data=CrypticCreatures,\n",
    "#                     groups=CrypticCreatures[\"id\"],\n",
    "#                     re_formula=\"1 + C(chosen_outcome_shift)\",\n",
    "#                     vc_formula={\"task_id\": \"0 + C(task_id)\"})\n",
    "\n",
    "# result = model.fit(method='lbfgs', maxiter=100000)\n",
    "# print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CrypticCreatures['chosen_outcome_shift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecfac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymer4.models import Lmer\n",
    "import pandas as pd\n",
    "\n",
    "# Convert columns to strings and then to categorical.\n",
    "# This ensures that the factor levels are strings, which is what R expects.\n",
    "CrypticCreatures['patientstatus'] = CrypticCreatures['patientstatus'].astype(str).astype('category')\n",
    "CrypticCreatures['chosen_outcome_shift'] = CrypticCreatures['chosen_outcome_shift'].astype(str).astype('category')\n",
    "CrypticCreatures['task_id'] = CrypticCreatures['task_id'].astype(str).astype('category')\n",
    "if 'gender' in CrypticCreatures.columns:\n",
    "    CrypticCreatures['gender'] = CrypticCreatures['gender'].astype(str).astype('category')\n",
    "\n",
    "# Remove any unused categories.\n",
    "CrypticCreatures['patientstatus'] = CrypticCreatures['patientstatus'].cat.remove_unused_categories()\n",
    "CrypticCreatures['chosen_outcome_shift'] = CrypticCreatures['chosen_outcome_shift'].cat.remove_unused_categories()\n",
    "CrypticCreatures['task_id'] = CrypticCreatures['task_id'].cat.remove_unused_categories()\n",
    "if 'gender' in CrypticCreatures.columns:\n",
    "    CrypticCreatures['gender'] = CrypticCreatures['gender'].cat.remove_unused_categories()\n",
    "\n",
    "# Print out the levels to verify.\n",
    "print(\"patientstatus levels:\", list(CrypticCreatures['patientstatus'].cat.categories))\n",
    "print(\"chosen_outcome_shift levels:\", list(CrypticCreatures['chosen_outcome_shift'].cat.categories))\n",
    "print(\"task_id levels:\", list(CrypticCreatures['task_id'].cat.categories))\n",
    "if 'gender' in CrypticCreatures.columns:\n",
    "    print(\"gender levels:\", list(CrypticCreatures['gender'].cat.categories))\n",
    "\n",
    "# Create the factors dictionary using the string levels.\n",
    "factors = {\n",
    "    'patientstatus': list(CrypticCreatures['patientstatus'].cat.categories),\n",
    "    'chosen_outcome_shift': list(CrypticCreatures['chosen_outcome_shift'].cat.categories),\n",
    "    'task_id': list(CrypticCreatures['task_id'].cat.categories)\n",
    "}\n",
    "if 'gender' in CrypticCreatures.columns:\n",
    "    factors['gender'] = list(CrypticCreatures['gender'].cat.categories)\n",
    "\n",
    "print(\"Factors dictionary:\", factors)\n",
    "\n",
    "# Specify and fit the model using pymer4.\n",
    "model = Lmer(\"confidence ~ patientstatus * chosen_outcome_shift + age.sc + gender + total_iq.sc + (1+chosen_outcome_shift|id) + (1|task_id)\",\n",
    "             data=CrypticCreatures)\n",
    "result = model.fit(factors=factors)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.testing import assert_frame_equal\n",
    "# Import the dataset\n",
    "testCrypticCreatures = pd.read_csv(\"testCrypticCreatures2.csv\")\n",
    "\n",
    "# Rename the existing CrypticCreatures to df1\n",
    "df1 = CrypticCreatures.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "\n",
    "# Rename the imported dataset to df2\n",
    "df2 = CrypticCreatures2.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "try:\n",
    "    # Use check_exact=False and set tolerances if you expect minor numerical differences.\n",
    "    assert_frame_equal(df1, df2, check_exact=False, rtol=1e-5, atol=1e-8)\n",
    "    print(\"The dataframes are equal!\")\n",
    "except AssertionError as e:\n",
    "    print(\"Differences found between dataframes:\\n\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd7607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datacompy\n",
    "\n",
    "# Define join columns\n",
    "join_cols = ['id', 'task_id', 'run', 'trial']\n",
    "\n",
    "# Ensure key columns are stripped and converted to numeric\n",
    "for col in join_cols:\n",
    "    # Remove leading/trailing spaces (if any)\n",
    "    df1[col] = df1[col].astype(str).str.strip()\n",
    "    df2[col] = df2[col].astype(str).str.strip()\n",
    "    \n",
    "    # Convert to numeric if applicable\n",
    "    df1[col] = pd.to_numeric(df1[col], errors='coerce')\n",
    "    df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "\n",
    "# Sort both DataFrames and reset the index\n",
    "df1_sorted = df1.sort_values(by=join_cols).reset_index(drop=True)\n",
    "df2_sorted = df2.sort_values(by=join_cols).reset_index(drop=True)\n",
    "\n",
    "# Check sorted order\n",
    "print(\"df1_sorted head:\")\n",
    "print(df1_sorted[join_cols].head(20))\n",
    "print(\"df2_sorted head:\")\n",
    "print(df2_sorted[join_cols].head(20))\n",
    "\n",
    "# Compare using datacompy\n",
    "compare = datacompy.Compare(\n",
    "    df1_sorted, df2_sorted,\n",
    "    join_columns=join_cols,\n",
    "    abs_tol=1e-5,\n",
    "    rel_tol=1e-5,\n",
    "    df1_name='df1_sorted',\n",
    "    df2_name='df2_sorted'\n",
    ")\n",
    "print(compare.report())\n",
    "# Get the unique id values from each DataFrame\n",
    "ids_df1 = set(df1['id'].unique())\n",
    "ids_df2 = set(df2['id'].unique())\n",
    "\n",
    "# Compare the sets\n",
    "if ids_df1 == ids_df2:\n",
    "    print(\"Both DataFrames have the same unique id values.\")\n",
    "else:\n",
    "    print(\"The DataFrames have different unique id values.\")\n",
    "    # Show differences if needed:\n",
    "    print(\"IDs in df1 but not in df2:\", ids_df1 - ids_df2)\n",
    "    print(\"IDs in df2 but not in df1:\", ids_df2 - ids_df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's assume df1 and df2 are your two DataFrames.\n",
    "# Ensure they are sorted by 'id' (or by the full key if needed)\n",
    "df1_sorted = df1.sort_values(by=['id']).reset_index(drop=True)\n",
    "df2_sorted = df2.sort_values(by=['id']).reset_index(drop=True)\n",
    "\n",
    "# Merge the DataFrames on 'id'. If id is not unique, you'll need a more complex key.\n",
    "merged = pd.merge(df1_sorted, df2_sorted, on='id', suffixes=('_df1', '_df2'))\n",
    "\n",
    "# List the variables in your model:\n",
    "model_vars = [\n",
    "    \"confidence.sc\", \n",
    "    \"patientstatus\", \n",
    "    \"chosen_outcome_shift\", \n",
    "    \"age.sc\", \n",
    "    \"gender\", \n",
    "    \"total_iq.sc\", \n",
    "    \"task_id\"\n",
    "]\n",
    "\n",
    "# Compare each variable row by row:\n",
    "for var in model_vars:\n",
    "    col1 = f\"{var}_df1\"\n",
    "    col2 = f\"{var}_df2\"\n",
    "    if col1 in merged.columns and col2 in merged.columns:\n",
    "        # Create a mask where the two columns differ. For numeric columns, you might\n",
    "        # want to include a tolerance if needed.\n",
    "        diff_mask = merged[col1] != merged[col2]\n",
    "        \n",
    "        # For numeric columns, you can use np.isclose for tolerance comparisons:\n",
    "        if pd.api.types.is_numeric_dtype(merged[col1]):\n",
    "            diff_mask = ~np.isclose(merged[col1], merged[col2], rtol=1e-5, atol=1e-8)\n",
    "        \n",
    "        mismatches = merged.loc[diff_mask, ['id', col1, col2]]\n",
    "        \n",
    "        if mismatches.empty:\n",
    "            print(f\"All rows match for variable {var}.\")\n",
    "        else:\n",
    "            print(f\"Mismatches found for variable {var}:\")\n",
    "            print(mismatches)\n",
    "    else:\n",
    "        print(f\"Variable {var} not found in both DataFrames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3291d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, for chosen_outcome_shift:\n",
    "df1['chosen_outcome_shift'] = pd.to_numeric(df1['chosen_outcome_shift'], errors='coerce').round(5)\n",
    "df2['chosen_outcome_shift'] = pd.to_numeric(df2['chosen_outcome_shift'], errors='coerce').round(5)\n",
    "\n",
    "# For categorical variables, convert them to strings\n",
    "for var in ['patientstatus', 'gender']:\n",
    "    df1[var] = df1[var].astype(str).str.strip()\n",
    "    df2[var] = df2[var].astype(str).str.strip()\n",
    "\n",
    "# Do similar conversions for any other columns (like confidence.sc, age.sc, total_iq.sc)\n",
    "model_vars = [\"confidence.sc\", \"patientstatus\", \"chosen_outcome_shift\", \"age.sc\", \"gender\", \"total_iq.sc\", \"id\", \"task_id\"]\n",
    "\n",
    "for var in model_vars:\n",
    "    if var not in df1.columns:\n",
    "        print(f\"{var} not found in df1\")\n",
    "    if var not in df2.columns:\n",
    "        print(f\"{var} not found in df2\")\n",
    "join_cols = ['id', 'task_id', 'run', 'trial']\n",
    "df1_sorted = df1.sort_values(by=join_cols).reset_index(drop=True)\n",
    "df2_sorted = df2.sort_values(by=join_cols).reset_index(drop=True)\n",
    "\n",
    "merged = pd.merge(df1_sorted, df2_sorted, on=join_cols, suffixes=('_df1', '_df2'))\n",
    "pd.set_option('display.max_rows', 100)  # or None to show all\n",
    "\n",
    "for var in model_vars:\n",
    "    col1 = f\"{var}_df1\"\n",
    "    col2 = f\"{var}_df2\"\n",
    "    if col1 in merged.columns and col2 in merged.columns:\n",
    "        # For numeric columns, compare with tolerance using np.isclose.\n",
    "        if pd.api.types.is_numeric_dtype(merged[col1]):\n",
    "            diff_mask = ~np.isclose(merged[col1], merged[col2], rtol=1e-5, atol=1e-8)\n",
    "        else:\n",
    "            diff_mask = merged[col1] != merged[col2]\n",
    "        mismatches = merged.loc[diff_mask, [*join_cols, col1, col2]]\n",
    "        if mismatches.empty:\n",
    "            print(f\"All rows match for variable {var}.\")\n",
    "        else:\n",
    "            print(f\"\\nMismatches found for variable {var}:\")\n",
    "            print(mismatches)\n",
    "    else:\n",
    "        print(f\"Variable {var} not found in both DataFrames.\")\n",
    "import datacompy\n",
    "\n",
    "compare = datacompy.Compare(\n",
    "    df1_sorted, df2_sorted,\n",
    "    join_columns=join_cols,\n",
    "    abs_tol=1e-5,\n",
    "    rel_tol=1e-5,\n",
    "    df1_name='df1_sorted',\n",
    "    df2_name='df2_sorted'\n",
    ")\n",
    "print(compare.report())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df29dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the key columns for matching (excluding chosen_outcome)\n",
    "join_cols = ['id', 'task_id', 'run', 'trial']\n",
    "\n",
    "# Ensure the join columns are numeric (if appropriate)\n",
    "for col in join_cols:\n",
    "    df1[col] = pd.to_numeric(df1[col], errors='coerce')\n",
    "    df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "\n",
    "# Sort both DataFrames by the join columns and reset the index\n",
    "df1_sorted = df1.sort_values(by=join_cols).reset_index(drop=True)\n",
    "df2_sorted = df2.sort_values(by=join_cols).reset_index(drop=True)\n",
    "\n",
    "# Merge the DataFrames on the key columns, adding suffixes to distinguish chosen_outcome from each dataset\n",
    "merged = pd.merge(df1_sorted, df2_sorted, on=join_cols, suffixes=('_df1', '_df2'))\n",
    "\n",
    "# At this point, the merged DataFrame should contain two columns:\n",
    "#   - chosen_outcome_df1: from the first dataset (df1)\n",
    "#   - chosen_outcome_df2: from the second dataset (df2)\n",
    "#\n",
    "# Optionally, if these columns should be numeric, convert and round them for consistency:\n",
    "merged['chosen_outcome_df1'] = pd.to_numeric(merged['chosen_outcome_df1'], errors='coerce').round(5)\n",
    "merged['chosen_outcome_df2'] = pd.to_numeric(merged['chosen_outcome_df2'], errors='coerce').round(5)\n",
    "\n",
    "# Create a DataFrame containing the join keys and the chosen_outcome variables from both datasets\n",
    "comparison = merged[join_cols + ['chosen_outcome_df1', 'chosen_outcome_df2']]\n",
    "\n",
    "# Export the entire comparison to CSV\n",
    "comparison.to_csv(\"comparison_chosen_outcome.csv\", index=False)\n",
    "print(\"Comparison saved to comparison_chosen_outcome.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c4d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def get_repo_root():\n",
    "    try:\n",
    "        # This command returns the absolute path of the repository root.\n",
    "        repo_root = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"--show-toplevel\"], stderr=subprocess.STDOUT\n",
    "        ).strip().decode(\"utf-8\")\n",
    "        return repo_root\n",
    "    except subprocess.CalledProcessError:\n",
    "        # If not in a git repository, fall back to current working directory.\n",
    "        return os.getcwd()\n",
    "\n",
    "repo_root = get_repo_root()\n",
    "print(\"Repository Root:\", repo_root)\n",
    "\n",
    "def read_csv_files(directory):\n",
    "    csv_files = []\n",
    "    print(f\"Checking files in directory: {directory}\")\n",
    "    for filename in os.listdir(directory):\n",
    "        print(f\"Found file: {filename}\")\n",
    "        if filename.startswith(\"sub\") and filename.endswith(\".csv\"):\n",
    "            print(f\"Reading CSV file: {filename}\")\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            csv_files.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping file: {filename}\")\n",
    "    return csv_files\n",
    "\n",
    "def summarySE(df, measurevar, groupvars, na_rm=True):\n",
    "    \"\"\"Compute mean, standard error, and count of observations for each group.\"\"\"\n",
    "    # Group by the specified columns\n",
    "    grouped = df.groupby(groupvars).agg(\n",
    "        mean=(measurevar, 'mean'),\n",
    "        count=(measurevar, 'size'),\n",
    "        std=(measurevar, 'std')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate standard error\n",
    "    grouped['se'] = grouped['std'] / np.sqrt(grouped['count'])\n",
    "    \n",
    "    # Remove rows with NaNs if na_rm is True\n",
    "    if na_rm:\n",
    "        grouped = grouped.dropna()\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "# Set the directory path\n",
    "directory = os.path.join(repo_root, \"data\")\n",
    "\n",
    "# Call the function and get the list of dataframes\n",
    "dataframes = read_csv_files(directory)\n",
    "\n",
    "# Display the first few rows of each DataFrame\n",
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"First few rows of DataFrame {i+1}:\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8472f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to the data folder within the repository.\n",
    "data_dir = os.path.join(repo_root, \"data\")\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Load data\n",
    "CrypticCreatures = pd.read_csv(\"Table_CrypticCreatures_YaleCohort.csv\")\n",
    "CrypticCreatures.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "CrypticCreatures2 = pd.read_csv(\"Table_CrypticCreatures_YaleCohort2.csv\")\n",
    "CrypticCreatures2 = CrypticCreatures2.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "CrypticCreature_relativeShift = pd.read_csv(\"Table_CrypticCreaturesShiftRelative_YaleCohort.csv\")\n",
    "CrypticCreatures_patients_relativeShift = pd.read_csv(\"Table_CrypticCreaturesShiftRelative_patients_YaleCohort.csv\")\n",
    "CrypticCreatures_patients_relativeShift = CrypticCreatures_patients_relativeShift.sort_values(by=['id', 'nTrial_rel'])\n",
    "CrypticCreatures_controls_relativeShift = pd.read_csv(\"Table_CrypticCreaturesShiftRelative_controls_YaleCohort.csv\")\n",
    "CrypticCreatures_controls_relativeShift = CrypticCreatures_controls_relativeShift.sort_values(by=['id', 'nTrial_rel'])\n",
    "\n",
    "CrypticCreatures_BayesianLearner = pd.read_csv(\"CrypticCreatures_BayesianLearner.csv\")\n",
    "CrypticCreatures_BayesianLearner = CrypticCreatures_BayesianLearner.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "CrypticCreatures_BayesianLearner_patients_relativeShift = pd.read_csv(\"CrypticCreaturesBayesianLearner_relativeShift_OCD.csv\")\n",
    "CrypticCreatures_BayesianLearner_patients_relativeShift = CrypticCreatures_BayesianLearner_patients_relativeShift.sort_values(by=['id', 'nTrial_rel'])\n",
    "CrypticCreatures_BayesianLearner_controls_relativeShift = pd.read_csv(\"CrypticCreaturesBayesianLearner_relativeShift_controls.csv\")\n",
    "CrypticCreatures_BayesianLearner_controls_relativeShift = CrypticCreatures_BayesianLearner_controls_relativeShift.sort_values(by=['id', 'nTrial_rel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming CrypticCreatures_patients_relativeShift is a DataFrame\n",
    "Cryptic_mean_acc_ID_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_accuracy_id', ['nTrial_rel'])\n",
    "Cryptic_mean_acc_ED_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_accuracy_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_acc_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_accuracy', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_conf_ID_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_confidence_id', ['nTrial_rel'])\n",
    "Cryptic_mean_conf_ED_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_confidence_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_conf_patients = summarySE(CrypticCreatures_patients_relativeShift, 'mean_confidence', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_acc_ID_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_accuracy_id', ['nTrial_rel'])\n",
    "Cryptic_mean_acc_ED_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_accuracy_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_acc_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_accuracy', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_conf_ID_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_confidence_id', ['nTrial_rel'])\n",
    "Cryptic_mean_conf_ED_controls = summarySE(CrypticCreatures_controls_relativeShift, 'mean_confidence_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_conf_controls= summarySE(CrypticCreatures_controls_relativeShift, 'mean_confidence', ['nTrial_rel'])\n",
    "\n",
    "#Cryptic_mean_entr_ID_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeID, 'mean_entropy_id', ['nTrial_rel'])\n",
    "#Cryptic_mean_entr_ED_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeED, 'mean_entropy_ed', ['nTrial_rel'])\n",
    "Cryptic_mean_entr_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'entropy', ['nTrial_rel'])\n",
    "Cryptic_mean_entr_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'entropy', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_sumprior_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'sum_prior_chosen_features', ['nTrial_rel'])\n",
    "Cryptic_mean_sumprior_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'sum_prior_chosen_features', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_BLR_confidence_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'BLR_confidence', ['nTrial_rel'])\n",
    "Cryptic_mean_BLR_confidence_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'BLR_confidence', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_signed_confidence_deviation_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'signed_confidence_deviation', ['nTrial_rel'])\n",
    "Cryptic_mean_signed_confidence_deviation_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'signed_confidence_deviation', ['nTrial_rel'])\n",
    "\n",
    "Cryptic_mean_signed_prior_deviation_patients = summarySE(CrypticCreatures_BayesianLearner_patients_relativeShift, 'signed_prior_deviation', ['nTrial_rel'])\n",
    "Cryptic_mean_signed_prior_deviation_controls = summarySE(CrypticCreatures_BayesianLearner_controls_relativeShift, 'signed_prior_deviation', ['nTrial_rel'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d9a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a column for feedback type (1 for correct, 0 for incorrect)\n",
    "# CrypticCreatures['feedback'] = CrypticCreatures['chosen_outcome'].apply(lambda x: 1 if x == 'correct' else 0)\n",
    "\n",
    "# # Create lagged columns for feedback and confidence\n",
    "# CrypticCreatures['prev_feedback'] = CrypticCreatures['feedback'].shift(1)\n",
    "# CrypticCreatures['prev_confidence'] = CrypticCreatures['confidence'].shift(1)\n",
    "\n",
    "# # Filter out the first trial as it has no previous feedback\n",
    "# CrypticCreatures = CrypticCreatures.dropna(subset=['prev_feedback', 'prev_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a94bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summary_rel(df, measurevar, groupvar):\n",
    "    \"\"\"\n",
    "    Calculate mean, standard error, and count for a given measure variable.\n",
    "    \"\"\"\n",
    "    summary = df.groupby(groupvar).agg(\n",
    "        mean=(measurevar, 'mean'),\n",
    "        count=(measurevar, 'size'),\n",
    "        std=(measurevar, 'std')\n",
    "    ).reset_index()\n",
    "    summary['stderr'] = summary['std'] / np.sqrt(summary['count'])\n",
    "    return summary\n",
    "\n",
    "# data frames\n",
    "controls_df = CrypticCreatures_controls_relativeShift \n",
    "patients_df = CrypticCreatures_patients_relativeShift\n",
    "\n",
    "# Merge controls dataset based on 'nTrial_rel' and 'id'\n",
    "controls_merged_df = pd.merge(\n",
    "    CrypticCreatures_BayesianLearner_controls_relativeShift,\n",
    "    controls_df,\n",
    "    on=['nTrial_rel', 'id'],  # Merge on both nTrial_rel and id\n",
    "    how='inner'  # Use inner join to ensure matching nTrial_rel and id\n",
    ")\n",
    "\n",
    "# Merge patients dataset based on 'nTrial_rel' and 'id'\n",
    "patients_merged_df = pd.merge(\n",
    "    CrypticCreatures_BayesianLearner_patients_relativeShift,\n",
    "    patients_df,\n",
    "    on=['nTrial_rel', 'id'],  # Merge on both nTrial_rel and id\n",
    "    how='inner'  # Use inner join to ensure matching nTrial_rel and id\n",
    ")\n",
    "# List of measure variables for controls and patients\n",
    "measure_vars = [\n",
    "    'change_in_mean_accuracy', 'change_in_mean_accuracy_abs', \n",
    "    'change_in_mean_accuracy_ed', 'change_in_mean_accuracy_abs_ed', \n",
    "    'change_in_mean_accuracy_id', 'change_in_mean_accuracy_abs_id',\n",
    "    'change_in_mean_confidence', 'change_in_mean_confidence_abs',\n",
    "    'change_in_mean_confidence_ed', 'change_in_mean_confidence_abs_ed',\n",
    "    'change_in_mean_confidence_id', 'change_in_mean_confidence_abs_id',\n",
    "    'signed_confidence_deviation','signed_prior_deviation','mean_confidence','BLR_confidence',\n",
    "    'mean_accuracy'\n",
    "    \n",
    "]\n",
    "\n",
    "# Calculate summaries for controls and patients\n",
    "summary_controls_rel = {var: calculate_summary_rel(controls_merged_df, var, 'nTrial_rel') for var in measure_vars}\n",
    "summary_patients_rel = {var: calculate_summary_rel(patients_merged_df, var, 'nTrial_rel') for var in measure_vars}\n",
    "print(summary_controls_rel['mean_confidence'])\n",
    "print(Cryptic_mean_conf_controls)\n",
    "# \n",
    "summary_controls_accuracy = summary_controls_rel['change_in_mean_accuracy']\n",
    "summary_patients_accuracy = summary_patients_rel['change_in_mean_accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def summarySE(data, measurevar, groupvars, na_rm=True):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a given measure variable.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame\n",
    "    - measurevar: str, the name of the measure variable\n",
    "    - groupvars: list of str, the names of the grouping variables\n",
    "    - na_rm: bool, whether to remove NA values\n",
    "    \n",
    "    Returns:\n",
    "    - summary: DataFrame with summary statistics\n",
    "    \"\"\"\n",
    "    # Remove NA values if na_rm is True\n",
    "    if na_rm:\n",
    "        data = data.dropna(subset=[measurevar])\n",
    "    \n",
    "    # Group by the specified variables and compute the summary statistics\n",
    "    summary = data.groupby(groupvars).agg(\n",
    "        mean=(measurevar, 'mean'),\n",
    "        count=(measurevar, 'size'),\n",
    "        std=(measurevar, 'std')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Compute the standard error\n",
    "    summary['se'] = summary['std'] / np.sqrt(summary['count'])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Assuming CrypticCreatures_controls_relativeShift and CrypticCreatures_patients_relativeShift are defined as DataFrames\n",
    "\n",
    "# Compute summary statistics for controls\n",
    "Cryptic_mean_confidence_controls = summarySE(\n",
    "    CrypticCreatures_controls_relativeShift, \n",
    "    measurevar=\"mean_confidence\", \n",
    "    groupvars=[\"nTrial_rel\"],\n",
    "    na_rm=True\n",
    ")\n",
    "\n",
    "# Compute summary statistics for patients\n",
    "Cryptic_mean_confidence_patients = summarySE(\n",
    "    CrypticCreatures_patients_relativeShift, \n",
    "    measurevar=\"mean_confidence\", \n",
    "    groupvars=[\"nTrial_rel\"],\n",
    "    na_rm=True\n",
    ")\n",
    "\n",
    "# Print the results to compare\n",
    "print(\"Summary statistics for controls:\")\n",
    "print(Cryptic_mean_confidence_controls)\n",
    "\n",
    "print(\"\\nSummary statistics for patients:\")\n",
    "print(Cryptic_mean_confidence_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, shapiro, levene, mannwhitneyu\n",
    "\n",
    "# Assuming CrypticCreatures is your dataset\n",
    "# CrypticCreatures = pd.read_csv('path_to_CrypticCreatures.csv')\n",
    "\n",
    "# Compute each individual's average accuracy and confidence\n",
    "CrypticCreatures['average_accuracy'] = CrypticCreatures.groupby('id')['chosen_outcome'].transform('mean')\n",
    "CrypticCreatures['average_confidence'] = CrypticCreatures.groupby('id')['confidence'].transform('mean')\n",
    "average_data = CrypticCreatures[['id', 'average_accuracy', 'average_confidence', 'patientstatus']].drop_duplicates()\n",
    "\n",
    "# Separate the groups\n",
    "controls_acc = average_data[average_data['patientstatus'] == 0]['average_accuracy']\n",
    "patients_acc = average_data[average_data['patientstatus'] == 1]['average_accuracy']\n",
    "controls_conf = average_data[average_data['patientstatus'] == 0]['average_confidence']\n",
    "patients_conf = average_data[average_data['patientstatus'] == 1]['average_confidence']\n",
    "\n",
    "# Function to perform tests and return p-value\n",
    "def perform_tests(controls, patients, measure_name):\n",
    "    # Check for normality\n",
    "    shapiro_controls = shapiro(controls)\n",
    "    shapiro_patients = shapiro(patients)\n",
    "    print(f'Shapiro-Wilk Test for Controls {measure_name}: {shapiro_controls}')\n",
    "    print(f'Shapiro-Wilk Test for Patients {measure_name}: {shapiro_patients}')\n",
    "    \n",
    "    # Check for homogeneity of variances\n",
    "    levene_test = levene(controls, patients)\n",
    "    print(f'Levene\\'s Test for {measure_name}: {levene_test}')\n",
    "    \n",
    "    # Perform t-test if assumptions are met, otherwise use Mann-Whitney U test\n",
    "    if shapiro_controls.pvalue > 0.05 and shapiro_patients.pvalue > 0.05 and levene_test.pvalue > 0.05:\n",
    "        t_stat, p_value = ttest_ind(controls, patients)\n",
    "        print(f'T-test for {measure_name}: T-statistic = {t_stat}, P-value = {p_value}')\n",
    "    else:\n",
    "        u_stat, p_value = mannwhitneyu(controls, patients)\n",
    "        print(f'Mann-Whitney U Test for {measure_name}: U-statistic = {u_stat}, P-value = {p_value}')\n",
    "    return p_value\n",
    "\n",
    "# Perform tests and get p-values\n",
    "p_value_acc = perform_tests(controls_acc, patients_acc, 'Accuracy')\n",
    "p_value_conf = perform_tests(controls_conf, patients_conf, 'Confidence')\n",
    "\n",
    "# Function to draw significance bracket\n",
    "def draw_significance_bracket(ax, x1, x2, y, text, height_percent=0.02):\n",
    "    height = y * height_percent\n",
    "    ax.plot([x1, x1, x2, x2], [y, y + height, y + height, y], lw=1.5, color='black')\n",
    "    ax.plot([x1, x1], [y, y - height], lw=1.5, color='black')  # Left vertical line\n",
    "    ax.plot([x2, x2], [y, y - height], lw=1.5, color='black')  # Right vertical line\n",
    "    ax.text((x1 + x2) * 0.5, y + height * 1.5, text, ha='center', va='bottom', color='black', fontsize=20)\n",
    "\n",
    "# Determine significance text based on p-value\n",
    "def get_significance_text(p_value):\n",
    "    if p_value < 0.001:\n",
    "        return '***'\n",
    "    elif p_value < 0.01:\n",
    "        return '**'\n",
    "    elif p_value < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'n.s.'\n",
    "\n",
    "# Plotting function for accuracy\n",
    "def plot_accuracy_boxplot(average_data, p_value_acc):\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    ax = sns.boxplot(x='patientstatus', y='average_accuracy', data=average_data, palette={0: '#4F67B1' , 1: '#B44B28'})\n",
    "    sns.swarmplot(x='patientstatus', y='average_accuracy', data=average_data, color='black', dodge=True, ax=ax, marker='o', size=5)\n",
    "    \n",
    "    # Add significance indication for accuracy\n",
    "    max_acc = max(average_data['average_accuracy'])\n",
    "    ylim = max_acc + 0.1\n",
    "    significance_text = get_significance_text(p_value_acc)\n",
    "    draw_significance_bracket(ax, 0, 1, ylim - 0.07, significance_text)\n",
    "    \n",
    "    ax.set_ylim(0.4, ylim)\n",
    "    ax.set_xticklabels(['Controls', 'Patients'])\n",
    "    ax.set_yticks(np.arange(0.5, 0.9, 0.1))\n",
    "    ax.set_title('Average Accuracy Comparison')\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_ylabel('Average Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting function for confidence\n",
    "def plot_confidence_boxplot(average_data, p_value_conf):\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    ax = sns.boxplot(x='patientstatus', y='average_confidence', data=average_data, palette={0: '#4F67B1' , 1: '#B44B28'})\n",
    "    sns.swarmplot(x='patientstatus', y='average_confidence', data=average_data, color='black', dodge=True, ax=ax, marker='o', size=5)\n",
    "    \n",
    "    # Add significance indication for confidence\n",
    "    max_conf = max(average_data['average_confidence'])\n",
    "    ylim = max_conf + 10\n",
    "    significance_text = get_significance_text(p_value_conf)\n",
    "    draw_significance_bracket(ax, 0, 1, ylim - 7, significance_text)\n",
    "    \n",
    "    ax.set_ylim(0, ylim)\n",
    "    ax.set_xticklabels(['Controls', 'Patients'])\n",
    "    ax.set_yticks(np.arange(0, 121, 20))\n",
    "    ax.set_title('Average Confidence Comparison')\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_ylabel('Average Confidence')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the boxplots separately\n",
    "plot_accuracy_boxplot(average_data, p_value_acc)\n",
    "plot_confidence_boxplot(average_data, p_value_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d39a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_var(summary_controls, summary_patients, measurevar, title, colors):\n",
    "    \"\"\"\n",
    "    Plot the change in mean var across shifts for controls and patients.\n",
    "    \"\"\"\n",
    "    # Add group labels\n",
    "    summary_controls['patientstatus'] = 0\n",
    "    summary_patients['patientstatus'] = 1\n",
    "    \n",
    "    # Combine datasets\n",
    "    df_combined = pd.concat([summary_controls, summary_patients], ignore_index=True)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for group in [0, 1]:\n",
    "        subset = df_combined[df_combined['patientstatus'] == group]\n",
    "        \n",
    "        # Plot line and scatter points\n",
    "        sns.lineplot(data=subset, x='nTrial_rel', y='mean', color=colors[group])\n",
    "        sns.scatterplot(data=subset, x='nTrial_rel', y='mean', color=colors[group], edgecolor=colors[group], s=100, label='Controls' if group == 0 else 'Patients')\n",
    "\n",
    "        # Add error bars\n",
    "        plt.errorbar(subset['nTrial_rel'], subset['mean'], yerr=subset['stderr'], fmt='o', color=colors[group], capsize=5)\n",
    "    \n",
    "    #plt.axhline(0, color='black', linewidth=1.2, linestyle='--')\n",
    "    plt.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Trial (0=Shifts)')\n",
    "    plt.ylabel('Mean Accuracy')\n",
    "    plt.legend(title='Group')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d93c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Optionally, update the global rcParams to set the minimum font size:\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# ============\n",
    "# (1) Adapted Plot Functions that accept an axis\n",
    "# ============\n",
    "def draw_significance_bracket(ax, x1, x2, y, text, height_percent=0.01):\n",
    "    height = y * height_percent\n",
    "    ax.plot([x1, x1, x2, x2], [y, y + height, y + height, y], lw=1.5, color='black')\n",
    "    ax.plot([x1, x1], [y, y - height], lw=1.5, color='black')  # Left vertical line\n",
    "    ax.plot([x2, x2], [y, y - height], lw=1.5, color='black')  # Right vertical line\n",
    "    # Significance text is set to 16 (min)\n",
    "    ax.text((x1 + x2) * 0.5, y + height * 1.5, text, ha='center', va='bottom', \n",
    "            color='black', fontsize=16)\n",
    "\n",
    "def get_significance_text(p_value):\n",
    "    if p_value < 0.001:\n",
    "        return '***'\n",
    "    elif p_value < 0.01:\n",
    "        return '**'\n",
    "    elif p_value < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'n.s.'\n",
    "    \n",
    "def plot_accuracy_boxplot_ax(ax, average_data, p_value_acc):\n",
    "    # Boxplot and swarmplot:\n",
    "    sns.boxplot(x='patientstatus', y='average_accuracy', data=average_data, \n",
    "                palette={0: '#4F67B1' , 1: '#B44B28'}, ax=ax, width=0.3, linewidth=1.5)\n",
    "    sns.stripplot(x='patientstatus', y='average_accuracy', data=average_data, \n",
    "                  color='#222328', jitter=True, ax=ax, marker='o', size=4)\n",
    "    # Determine y-limit and add significance bracket:\n",
    "    max_acc = average_data['average_accuracy'].max()\n",
    "    ylim = max_acc + 0.1\n",
    "    significance_text = get_significance_text(p_value_acc)\n",
    "    draw_significance_bracket(ax, 0, 1, ylim - 0.07, significance_text)\n",
    "    \n",
    "    # Format the axis:\n",
    "    ax.set_ylim(0.4, ylim)\n",
    "    ax.set_xticks([0.5])\n",
    "    ax.set_xticklabels(['Overall'])\n",
    "    ax.set_yticks(np.arange(0.5, 0.9, 0.1))\n",
    "    ax.set_xlabel(' ')\n",
    "    # Increase y-label font size to 18:\n",
    "    ax.set_ylabel('Average Accuracy', fontweight='bold', fontsize=16)\n",
    "    # Set tick label size to at least 16:\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "    \n",
    "    sns.despine()\n",
    "\n",
    "def plot_confidence_boxplot_ax(ax, average_data, p_value_conf):\n",
    "    sns.boxplot(x='patientstatus', y='average_confidence', data=average_data, \n",
    "                palette={0: '#4F67B1' , 1: '#B44B28'}, ax=ax, width=0.3,linewidth=1.5)\n",
    "    sns.stripplot(x='patientstatus', y='average_confidence', data=average_data, \n",
    "                  color='#222328', jitter=True, ax=ax, marker='o', size=4)\n",
    "    \n",
    "    max_conf = average_data['average_confidence'].max()\n",
    "    ylim = max_conf + 10\n",
    "    significance_text = get_significance_text(p_value_conf)\n",
    "    draw_significance_bracket(ax, 0, 1, ylim - 7, significance_text)\n",
    "    \n",
    "    ax.set_ylim(0, ylim)\n",
    "    ax.set_xticks([0.5])\n",
    "    ax.set_xticklabels(['Overall'])\n",
    "    ax.set_yticks(np.arange(0, 121, 20))\n",
    "    ax.set_xlabel(' ')\n",
    "    ax.set_ylabel('Average Confidence', fontweight='bold', fontsize=16)\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "    \n",
    "    sns.despine()\n",
    "\n",
    "def plot_mean_var_ax(ax, summary_controls, summary_patients, measurevar, ylabel, xlabel, ylim, yticks, colors):\n",
    "    # Convert inputs to DataFrames and add a group label:\n",
    "    summary_controls = pd.DataFrame(summary_controls)\n",
    "    summary_patients = pd.DataFrame(summary_patients)\n",
    "    summary_controls['patientstatus'] = 'Controls'\n",
    "    summary_patients['patientstatus'] = 'Patients'\n",
    "    \n",
    "    df_combined = pd.concat([summary_controls, summary_patients], ignore_index=True)\n",
    "    ax.axvline(0, color='black', linewidth=1, linestyle='--')\n",
    "    # Plot lines, scatter points, and error bars:\n",
    "    for group_key, color in colors.items():\n",
    "        label = 'Controls' if group_key == 0 else 'Patients'\n",
    "        subset = df_combined[df_combined['patientstatus'] == label]\n",
    "        \n",
    "        sns.lineplot(data=subset, x='nTrial_rel', y='mean', color=color, ax=ax)\n",
    "        sns.scatterplot(data=subset, x='nTrial_rel', y='mean', color=color, \n",
    "                        edgecolor='white', s=100, label=label, ax=ax)\n",
    "        ax.errorbar(subset['nTrial_rel'], subset['mean'], yerr=subset['stderr'], \n",
    "                    fmt='none', color=color, capsize=5)\n",
    "    \n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_xticks(np.arange(-5, 6, 1))\n",
    "    \n",
    "    # Increase x- and y-label font sizes to 18:\n",
    "    ax.set_xlabel(' ', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel(ylabel, fontsize=16)\n",
    "    ax.set_xlabel(xlabel, fontsize=16)\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "    # Update legend font sizes:\n",
    "    if measurevar == 'mean_accuracy':\n",
    "        ax.legend(title='Group', fontsize=14, title_fontsize=14, loc='lower left')\n",
    "    else:\n",
    "        legend = ax.get_legend()\n",
    "        if legend is not None:\n",
    "            legend.remove()\n",
    "    sns.despine()\n",
    "\n",
    "# ============\n",
    "# (2) Create the Combined Figure\n",
    "# ============\n",
    "# (Note: average_data, p_value_acc, p_value_conf, summary_controls_rel, \n",
    "# and summary_patients_rel should be defined in your workspace.)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    2, 2, \n",
    "    figsize=(10, 10), \n",
    "    sharey='row', \n",
    "    gridspec_kw={'width_ratios': [1, 2], 'wspace': 0}\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(wspace=0)\n",
    "\n",
    "# Plot accuracy boxplot and mean variance plot:\n",
    "plot_accuracy_boxplot_ax(axes[0, 0], average_data, p_value_acc)\n",
    "plot_mean_var_ax(axes[0, 1], \n",
    "                 summary_controls_rel['mean_accuracy'], \n",
    "                 summary_patients_rel['mean_accuracy'], \n",
    "                 'mean_accuracy', 'Mean Accuracy',' ',\n",
    "                 (0, 1), np.arange(0, 1.1, 0.2), \n",
    "                 colors={0: '#4F67B1' , 1: '#B44B28'})\n",
    "\n",
    "# Add 'A' label to the upper left corner of the first row of plots\n",
    "axes[0, 0].text(-0.1, 1.1, 'A', transform=axes[0, 0].transAxes, \n",
    "                fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "\n",
    "# Plot confidence boxplot and mean variance plot:\n",
    "plot_confidence_boxplot_ax(axes[1, 0], average_data, p_value_conf)\n",
    "plot_mean_var_ax(axes[1, 1], \n",
    "                 summary_controls_rel['mean_confidence'], \n",
    "                 summary_patients_rel['mean_confidence'], \n",
    "                 'mean_confidence', 'Mean Confidence', 'Trial (0=Shifts)',\n",
    "                 (20, 115), np.arange(20, 110, 20), \n",
    "                 colors={0: '#4F67B1' , 1: '#B44B28'})\n",
    "\n",
    "# Add 'B' label to the upper left corner of the second row of plots\n",
    "axes[1, 0].text(-0.1, 1.1, 'B', transform=axes[1, 0].transAxes, \n",
    "                fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Create the subfolder if it doesn't exist\n",
    "if not os.path.exists('../figures'):\n",
    "    os.makedirs('../figures')\n",
    "\n",
    "# Save the figure into the subfolder\n",
    "plt.savefig('../figures/Fig2_AccConf_Average.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50627a72",
   "metadata": {},
   "source": [
    "Below are plots to look at changes in variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind, shapiro, levene, mannwhitneyu\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Update global font size\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# --- Helper Functions for Significance Annotation ---\n",
    "def draw_significance_bracket(ax, x1, x2, y, text, height_percent=0.1):\n",
    "    height = y * height_percent\n",
    "    # Draw bracket\n",
    "    ax.plot([x1, x1, x2, x2], [y, y + height, y + height, y], lw=1.5, color='black')\n",
    "    # Place significance text\n",
    "    ax.text((x1 + x2) * 0.5, y + height * 1.5, text, ha='center', va='bottom', \n",
    "            color='black', fontsize=16)\n",
    "\n",
    "def get_significance_text(p_value):\n",
    "    if p_value < 0.001:\n",
    "        return '***'\n",
    "    elif p_value < 0.01:\n",
    "        return '**'\n",
    "    elif p_value < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'n.s.'\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Sort and create an overall trial variable (\"trial_all\")\n",
    "# -------------------------------\n",
    "sort_cols = ['id']\n",
    "for col in ['task_id', 'run', 'trial']:\n",
    "    if col in CrypticCreatures.columns:\n",
    "        sort_cols.append(col)\n",
    "CrypticCreatures = CrypticCreatures.sort_values(by=sort_cols)\n",
    "CrypticCreatures['trial_all'] = CrypticCreatures.groupby('id').cumcount() + 1\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Compute previous trial outcome and change in confidence\n",
    "# -------------------------------\n",
    "CrypticCreatures['prev_outcome'] = CrypticCreatures.groupby('id')['chosen_outcome'].shift(1)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Mark trials based on the previous trial's outcome\n",
    "# -------------------------------\n",
    "CrypticCreatures['trial_type'] = np.where(\n",
    "    CrypticCreatures['prev_outcome'] == 1, 'after_correct',\n",
    "    np.where(CrypticCreatures['prev_outcome'] == 0, 'after_incorrect', np.nan)\n",
    ")\n",
    "df_marked = CrypticCreatures.dropna(subset=['trial_type'])\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Aggregate the change scores per participant for each trial type\n",
    "# -------------------------------\n",
    "avg_change = (\n",
    "    df_marked.groupby(['id', 'trial_type'])['confidence_change']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "avg_change_pivot = avg_change.pivot(index='id', columns='trial_type', values='confidence_change').reset_index()\n",
    "avg_change_pivot = avg_change_pivot.drop('nan', axis=1)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Merge patient status information (assuming one value per participant)\n",
    "# -------------------------------\n",
    "patient_info = CrypticCreatures.groupby('id')['patientstatus'].first().reset_index()\n",
    "avg_change_pivot = pd.merge(avg_change_pivot, patient_info, on='id')\n",
    "print(avg_change_pivot.head())\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Group comparisons: tests for each change type (with assumption checks)\n",
    "# -------------------------------\n",
    "controls = avg_change_pivot[avg_change_pivot['patientstatus'] == 0]\n",
    "patients = avg_change_pivot[avg_change_pivot['patientstatus'] == 1]\n",
    "\n",
    "# Initialize significance p-value storage\n",
    "significance_correct = None\n",
    "significance_incorrect = None\n",
    "\n",
    "# --- After Correct Trials ---\n",
    "after_correct_controls = controls['after_correct'].dropna()\n",
    "after_correct_patients = patients['after_correct'].dropna()\n",
    "\n",
    "if len(after_correct_controls) > 0 and len(after_correct_patients) > 0:\n",
    "    # Check normality\n",
    "    shapiro_controls = shapiro(after_correct_controls)\n",
    "    shapiro_patients = shapiro(after_correct_patients)\n",
    "    # Check variance homogeneity\n",
    "    levene_test = levene(after_correct_controls, after_correct_patients)\n",
    "\n",
    "    print(\"After Correct Trials Assumptions:\")\n",
    "    print(f\"  Shapiro p-values: Controls={shapiro_controls.pvalue:.3f}, Patients={shapiro_patients.pvalue:.3f}\")\n",
    "    print(f\"  Levene p-value: {levene_test.pvalue:.3f}\")\n",
    "\n",
    "    if (shapiro_controls.pvalue > 0.05 and shapiro_patients.pvalue > 0.05) and (levene_test.pvalue > 0.05):\n",
    "        # Perform t-test\n",
    "        tstat_correct, pval_correct = ttest_ind(after_correct_controls, after_correct_patients)\n",
    "        print(\"After Correct Trials: t-test used\")\n",
    "        print(\"t = {:.3f}, p = {:.3f}\".format(tstat_correct, pval_correct))\n",
    "        significance_correct = pval_correct\n",
    "    else:\n",
    "        # Use nonparametric test\n",
    "        u_stat, pval_correct = mannwhitneyu(after_correct_controls, after_correct_patients, alternative='two-sided')\n",
    "        print(\"After Correct Trials: Mann-Whitney U test used\")\n",
    "        print(\"U = {:.3f}, p = {:.3f}\".format(u_stat, pval_correct))\n",
    "        significance_correct = pval_correct\n",
    "else:\n",
    "    print(\"Insufficient data for testing after correct trials.\")\n",
    "\n",
    "# --- After Incorrect Trials ---\n",
    "after_incorrect_controls = controls['after_incorrect'].dropna()\n",
    "after_incorrect_patients = patients['after_incorrect'].dropna()\n",
    "\n",
    "if len(after_incorrect_controls) > 0 and len(after_incorrect_patients) > 0:\n",
    "    # Check normality\n",
    "    shapiro_controls = shapiro(after_incorrect_controls)\n",
    "    shapiro_patients = shapiro(after_incorrect_patients)\n",
    "    # Check variance homogeneity\n",
    "    levene_test = levene(after_incorrect_controls, after_incorrect_patients)\n",
    "\n",
    "    print(\"\\nAfter Incorrect Trials Assumptions:\")\n",
    "    print(f\"  Shapiro p-values: Controls={shapiro_controls.pvalue:.3f}, Patients={shapiro_patients.pvalue:.3f}\")\n",
    "    print(f\"  Levene p-value: {levene_test.pvalue:.3f}\")\n",
    "\n",
    "    if (shapiro_controls.pvalue > 0.05 and shapiro_patients.pvalue > 0.05) and (levene_test.pvalue > 0.05):\n",
    "        # Perform t-test\n",
    "        tstat_incorrect, pval_incorrect = ttest_ind(after_incorrect_controls, after_incorrect_patients)\n",
    "        print(\"After Incorrect Trials: t-test used\")\n",
    "        print(\"t = {:.3f}, p = {:.3f}\".format(tstat_incorrect, pval_incorrect))\n",
    "        significance_incorrect = pval_incorrect\n",
    "    else:\n",
    "        # Use nonparametric test\n",
    "        u_stat, pval_incorrect = mannwhitneyu(after_incorrect_controls, after_incorrect_patients, alternative='two-sided')\n",
    "        print(\"After Incorrect Trials: Mann-Whitney U test used\")\n",
    "        print(\"U = {:.3f}, p = {:.3f}\".format(u_stat, pval_incorrect))\n",
    "        significance_incorrect = pval_incorrect\n",
    "else:\n",
    "    print(\"Insufficient data for testing after incorrect trials.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Plot boxplots of average change scores by patient status\n",
    "# -------------------------------\n",
    "sns.set_style(\"white\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "# Left plot: After Correct Trials\n",
    "sns.boxplot(\n",
    "    data=avg_change_pivot,\n",
    "    x='patientstatus', y='after_correct',\n",
    "    palette=[\"#4F67B1\", \"#B44B28\"],\n",
    "    ax=axes[0],\n",
    "    showfliers=False\n",
    ")\n",
    "sns.stripplot(\n",
    "    data=avg_change_pivot,\n",
    "    x='patientstatus', y='after_correct',\n",
    "    color=\"#222328\",\n",
    "    size=5,\n",
    "    jitter=True,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].axhline(y=0, linestyle=\"--\", color=\"#333333\", linewidth=1)\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[0].set_ylabel(\"Average Confidence Change\\nAfter Correct Trials\", fontsize=16, fontweight=\"bold\")\n",
    "axes[0].set_xticklabels([\" \", \" \"], fontsize=14)\n",
    "axes[0].tick_params(axis='y', labelsize=14)\n",
    "axes[0].tick_params(axis='x', bottom=None)\n",
    "axes[0].tick_params(axis='y', left=True)\n",
    "sns.despine(ax=axes[0])\n",
    "# Place label A above the plot\n",
    "axes[0].text(0.02, 1.03, \"A\", transform=axes[0].transAxes,\n",
    "             fontsize=20, fontweight=\"bold\", va=\"bottom\", ha=\"left\")\n",
    "\n",
    "# Right plot: After Incorrect Trials\n",
    "sns.boxplot(\n",
    "    data=avg_change_pivot,\n",
    "    x='patientstatus', y='after_incorrect',\n",
    "    palette=[\"#4F67B1\", \"#B44B28\"],\n",
    "    ax=axes[1],\n",
    "    showfliers=False\n",
    ")\n",
    "sns.stripplot(\n",
    "    data=avg_change_pivot,\n",
    "    x='patientstatus', y='after_incorrect',\n",
    "    color=\"#222328\",\n",
    "    size=5,\n",
    "    jitter=True,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].axhline(y=0, linestyle=\"--\", color=\"#333333\", linewidth=1)\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[1].set_ylabel(\"Average Confidence Change\\nAfter Incorrect Trials\", fontsize=16, fontweight=\"bold\")\n",
    "axes[1].set_xticklabels([\" \", \" \"], fontsize=14)\n",
    "axes[1].tick_params(axis='y', labelsize=14)\n",
    "axes[1].tick_params(axis='x', bottom=None)\n",
    "axes[1].tick_params(axis='y', left=True)\n",
    "sns.despine(ax=axes[1])\n",
    "# Place label B above the plot\n",
    "axes[1].text(0.02, 1.03, \"B\", transform=axes[1].transAxes,\n",
    "             fontsize=20, fontweight=\"bold\", va=\"bottom\", ha=\"left\")\n",
    "# Create legend handles\n",
    "control_patch = mlines.Line2D([], [], color=\"#4F67B1\", marker=\"s\", linestyle=\"None\", markersize=10, label=\"Control\")\n",
    "patient_patch = mlines.Line2D([], [], color=\"#B44B28\", marker=\"s\", linestyle=\"None\", markersize=10, label=\"Patient\")\n",
    "axes[0].legend(handles=[control_patch, patient_patch], title='Group', fontsize=14, title_fontsize=14, loc='lower left')\n",
    "# Ensure both plots share the same y-limits so the zero-line is aligned\n",
    "common_ylim = [min(axes[0].get_ylim()[0], axes[1].get_ylim()[0]),\n",
    "               max(axes[0].get_ylim()[1], axes[1].get_ylim()[1])+2]\n",
    "axes[0].set_ylim(common_ylim)\n",
    "axes[1].set_ylim(common_ylim)\n",
    "\n",
    "# --- Add significance brackets using the helper functions ---\n",
    "if significance_correct is not None:\n",
    "    sig_text = get_significance_text(significance_correct)\n",
    "    # Increase padding by adding a constant rather than a multiplier\n",
    "    y_max = avg_change_pivot['after_correct'].max() + 0.15\n",
    "    # Increase the bracket height using a higher height_percent value\n",
    "    draw_significance_bracket(axes[0], 0, 1, y_max, sig_text)\n",
    "    \n",
    "if significance_incorrect is not None:\n",
    "    sig_text = get_significance_text(significance_incorrect)\n",
    "    # Use the maximum y-value from the after_incorrect data with padding\n",
    "    y_max = avg_change_pivot['after_incorrect'].max() * 1.1\n",
    "    draw_significance_bracket(axes[1], 0, 1, y_max, sig_text)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5418c",
   "metadata": {},
   "source": [
    "Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd91d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import statsmodels.formula.api as smf\n",
    "\n",
    "# # Create some example data\n",
    "# df = pd.DataFrame({\n",
    "#     'y': [1, 2, 3, 4, 5],\n",
    "#     'x': [2, 1, 4, 3, 5]\n",
    "# })\n",
    "\n",
    "# # Fit a simple linear model\n",
    "# model = smf.ols(formula='y ~ x', data=df).fit()\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Define a helper function to compute z-scores within a group.\n",
    "def zscore_within(x):\n",
    "    m = x.mean()\n",
    "    s = x.std(ddof=0)\n",
    "    return (x - m) / s if s != 0 else x - m\n",
    "\n",
    "# ===========================================\n",
    "# 1. Create the .sc variables\n",
    "# ===========================================\n",
    "# Z-score the trial-level 'confidence' variable within each id.\n",
    "CrypticCreatures['confidence.sc'] = CrypticCreatures.groupby('id')['confidence'].transform(zscore_within)\n",
    "\n",
    "# For participant-level variables, perform global (across-person) z-scoring.\n",
    "if 'age' in CrypticCreatures.columns:\n",
    "    CrypticCreatures['age.sc'] = (CrypticCreatures['age'] - CrypticCreatures['age'].mean()) / CrypticCreatures['age'].std(ddof=0)\n",
    "if 'icar_totalscore' in CrypticCreatures.columns:\n",
    "    CrypticCreatures['total_iq.sc'] = (CrypticCreatures['icar_totalscore'] - CrypticCreatures['icar_totalscore'].mean()) / CrypticCreatures['icar_totalscore'].std(ddof=0)\n",
    "\n",
    "# ===========================================\n",
    "# 2. Ensure that categorical variables are of type 'category'\n",
    "# ===========================================\n",
    "CrypticCreatures['patientstatus'] = CrypticCreatures['patientstatus'].astype('category')\n",
    "CrypticCreatures['chosen_outcome_shift'] = CrypticCreatures['chosen_outcome_shift'].astype('category')\n",
    "CrypticCreatures['task_id'] = CrypticCreatures['task_id'].astype('category')\n",
    "\n",
    "# ===========================================\n",
    "# 3. Define and fit the mixed-effects regression model\n",
    "# ===========================================\n",
    "# # Wrap variable names with dots using Q()\n",
    "# formula = (\"Q('confidence.sc') ~ C(patientstatus)*C(chosen_outcome_shift) + Q('age.sc') + \"\n",
    "#            \"gender + Q('total_iq.sc')\")\n",
    "\n",
    "# # Fit the model with random effects:\n",
    "# #   - Random intercept and slope for chosen_outcome_shift within each id.\n",
    "# #   - Random intercept for task_id specified in vc_formula.\n",
    "# model = smf.mixedlm(formula,\n",
    "#                     data=CrypticCreatures,\n",
    "#                     groups=CrypticCreatures[\"id\"],\n",
    "#                     re_formula=\"1 + C(chosen_outcome_shift)\",\n",
    "#                     vc_formula={\"task_id\": \"0 + C(task_id)\"})\n",
    "\n",
    "# result = model.fit(method='lbfgs', maxiter=100000)\n",
    "# print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CrypticCreatures[['chosen_outcome_shift', 'chosen_outcome']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9305d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymer4.models import Lmer\n",
    "import pandas as pd\n",
    "\n",
    "# Convert columns to strings and then to categorical.\n",
    "# This ensures that the factor levels are strings, which is what R expects.\n",
    "CrypticCreatures['patientstatus'] = CrypticCreatures['patientstatus'].astype(str).astype('category')\n",
    "CrypticCreatures['chosen_outcome_shift'] = CrypticCreatures['chosen_outcome_shift'].astype(str).astype('category')\n",
    "CrypticCreatures['task_id'] = CrypticCreatures['task_id'].astype(str).astype('category')\n",
    "if 'gender' in CrypticCreatures.columns:\n",
    "    CrypticCreatures['gender'] = CrypticCreatures['gender'].astype(str).astype('category')\n",
    "\n",
    "# Remove any unused categories.\n",
    "CrypticCreatures['patientstatus'] = CrypticCreatures['patientstatus'].cat.remove_unused_categories()\n",
    "CrypticCreatures['chosen_outcome_shift'] = CrypticCreatures['chosen_outcome_shift'].cat.remove_unused_categories()\n",
    "CrypticCreatures['task_id'] = CrypticCreatures['task_id'].cat.remove_unused_categories()\n",
    "if 'gender' in CrypticCreatures.columns:\n",
    "    CrypticCreatures['gender'] = CrypticCreatures['gender'].cat.remove_unused_categories()\n",
    "\n",
    "# Print out the levels to verify.\n",
    "print(\"patientstatus levels:\", list(CrypticCreatures['patientstatus'].cat.categories))\n",
    "print(\"chosen_outcome_shift levels:\", list(CrypticCreatures['chosen_outcome_shift'].cat.categories))\n",
    "print(\"task_id levels:\", list(CrypticCreatures['task_id'].cat.categories))\n",
    "if 'gender' in CrypticCreatures.columns:\n",
    "    print(\"gender levels:\", list(CrypticCreatures['gender'].cat.categories))\n",
    "\n",
    "# Create the factors dictionary using the string levels.\n",
    "factors = {\n",
    "    'patientstatus': list(CrypticCreatures['patientstatus'].cat.categories),\n",
    "    'chosen_outcome_shift': list(CrypticCreatures['chosen_outcome_shift'].cat.categories),\n",
    "    'task_id': list(CrypticCreatures['task_id'].cat.categories)\n",
    "}\n",
    "if 'gender' in CrypticCreatures.columns:\n",
    "    factors['gender'] = list(CrypticCreatures['gender'].cat.categories)\n",
    "\n",
    "print(\"Factors dictionary:\", factors)\n",
    "\n",
    "# Specify and fit the model using pymer4.\n",
    "model = Lmer(\"confidence ~ patientstatus * chosen_outcome_shift + age.sc + gender + total_iq.sc + (1+chosen_outcome_shift|id) + (1|task_id)\",\n",
    "             data=CrypticCreatures)\n",
    "result = model.fit(factors=factors)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.testing import assert_frame_equal\n",
    "# Import the dataset\n",
    "testCrypticCreatures = pd.read_csv(\"testCrypticCreatures2.csv\")\n",
    "\n",
    "# Rename the existing CrypticCreatures to df1\n",
    "df1 = CrypticCreatures.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "\n",
    "# Rename the imported dataset to df2\n",
    "df2 = CrypticCreatures2.sort_values(by=['id', 'task_id', 'run', 'trial'])\n",
    "try:\n",
    "    # Use check_exact=False and set tolerances if you expect minor numerical differences.\n",
    "    assert_frame_equal(df1, df2, check_exact=False, rtol=1e-5, atol=1e-8)\n",
    "    print(\"The dataframes are equal!\")\n",
    "except AssertionError as e:\n",
    "    print(\"Differences found between dataframes:\\n\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aa7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datacompy\n",
    "\n",
    "# Define join columns\n",
    "join_cols = ['id', 'task_id', 'run', 'trial']\n",
    "\n",
    "# Ensure key columns are stripped and converted to numeric\n",
    "for col in join_cols:\n",
    "    # Remove leading/trailing spaces (if any)\n",
    "    df1[col] = df1[col].astype(str).str.strip()\n",
    "    df2[col] = df2[col].astype(str).str.strip()\n",
    "    \n",
    "    # Convert to numeric if applicable\n",
    "    df1[col] = pd.to_numeric(df1[col], errors='coerce')\n",
    "    df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "\n",
    "# Sort both DataFrames and reset the index\n",
    "df1_sorted = df1.sort_values(by=join_cols).reset_index(drop=True)\n",
    "df2_sorted = df2.sort_values(by=join_cols).reset_index(drop=True)\n",
    "\n",
    "# Check sorted order\n",
    "print(\"df1_sorted head:\")\n",
    "print(df1_sorted[join_cols].head(20))\n",
    "print(\"df2_sorted head:\")\n",
    "print(df2_sorted[join_cols].head(20))\n",
    "\n",
    "# Compare using datacompy\n",
    "compare = datacompy.Compare(\n",
    "    df1_sorted, df2_sorted,\n",
    "    join_columns=join_cols,\n",
    "    abs_tol=1e-5,\n",
    "    rel_tol=1e-5,\n",
    "    df1_name='df1_sorted',\n",
    "    df2_name='df2_sorted'\n",
    ")\n",
    "print(compare.report())\n",
    "# Get the unique id values from each DataFrame\n",
    "ids_df1 = set(df1['id'].unique())\n",
    "ids_df2 = set(df2['id'].unique())\n",
    "\n",
    "# Compare the sets\n",
    "if ids_df1 == ids_df2:\n",
    "    print(\"Both DataFrames have the same unique id values.\")\n",
    "else:\n",
    "    print(\"The DataFrames have different unique id values.\")\n",
    "    # Show differences if needed:\n",
    "    print(\"IDs in df1 but not in df2:\", ids_df1 - ids_df2)\n",
    "    print(\"IDs in df2 but not in df1:\", ids_df2 - ids_df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03554f00",
   "metadata": {},
   "source": [
    "Supplemental Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3243f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_change_in_mean_var(summary_controls, summary_patients, measurevar, title, ydesc, colors,ylims):\n",
    "    \"\"\"\n",
    "    Plot the change in mean var across shifts for controls and patients.\n",
    "    \"\"\"\n",
    "    # Add group labels\n",
    "    summary_controls['patientstatus'] = 0\n",
    "    summary_patients['patientstatus'] = 1\n",
    "    \n",
    "    # Combine datasets\n",
    "    df_combined = pd.concat([summary_controls, summary_patients], ignore_index=True)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for group in [0, 1]:\n",
    "        subset = df_combined[df_combined['patientstatus'] == group]\n",
    "        \n",
    "        # Plot line and scatter points\n",
    "        sns.lineplot(data=subset, x='nTrial_rel', y='mean', color=colors[group])\n",
    "        sns.scatterplot(data=subset, x='nTrial_rel', y='mean', color=colors[group], edgecolor=colors[group], s=100, label='Controls' if group == 0 else 'Patients')\n",
    "\n",
    "        # Add error bars\n",
    "        plt.errorbar(subset['nTrial_rel'], subset['mean'], yerr=subset['stderr'], fmt='o', color=colors[group], capsize=5)\n",
    "    \n",
    "    plt.axhline(0, color='black', linewidth=1.2, linestyle='--')\n",
    "    plt.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    plt.ylim(ylims)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Trial (0=Shifts)')\n",
    "    plt.ylabel(ydesc)\n",
    "    plt.legend(title='Group')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_change_in_mean_var(\n",
    "    summary_controls_rel['change_in_mean_accuracy'], \n",
    "    summary_patients_rel['change_in_mean_accuracy'], \n",
    "    'change_in_mean_accuracy', \n",
    "    ' ', \n",
    "    'Accuracy Delta',\n",
    "    colors = {0: 'olive', 1: 'darkblue'},\n",
    "    ylims = (-0.7, 0.4)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confidence \n",
    "plot_change_in_mean_var(\n",
    "    summary_controls_rel['change_in_mean_confidence'], \n",
    "    summary_patients_rel['change_in_mean_confidence'], \n",
    "    'change_in_mean_confidence', \n",
    "    ' ',\n",
    "    'Confidence Delta',\n",
    "     colors = {0: '#16463F', 1: '#2D80A7'},\n",
    "     ylims = (-19,6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5861bd40",
   "metadata": {},
   "source": [
    "Supplemental Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_change_in_mean_var(summary_controls, summary_patients, measurevar, title, ydesc, colors,ylims):\n",
    "    \"\"\"\n",
    "    Plot the change in mean var across shifts for controls and patients.\n",
    "    \"\"\"\n",
    "    # Add group labels\n",
    "    summary_controls['patientstatus'] = 0\n",
    "    summary_patients['patientstatus'] = 1\n",
    "    \n",
    "    # Combine datasets\n",
    "    df_combined = pd.concat([summary_controls, summary_patients], ignore_index=True)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for group in [0, 1]:\n",
    "        subset = df_combined[df_combined['patientstatus'] == group]\n",
    "        \n",
    "        # Plot line and scatter points\n",
    "        sns.lineplot(data=subset, x='nTrial_rel', y='mean', color=colors[group])\n",
    "        sns.scatterplot(data=subset, x='nTrial_rel', y='mean', color=colors[group], edgecolor=colors[group], s=100, label='Controls' if group == 0 else 'Patients')\n",
    "\n",
    "        # Add error bars\n",
    "        plt.errorbar(subset['nTrial_rel'], subset['mean'], yerr=subset['stderr'], fmt='o', color=colors[group], capsize=5)\n",
    "    \n",
    "    plt.axhline(0, color='black', linewidth=1.2, linestyle='--')\n",
    "    plt.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    plt.ylim(ylims)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Trial (0=Shifts)')\n",
    "    plt.ylabel(ydesc)\n",
    "    plt.legend(title='Group')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_change_in_mean_var(\n",
    "    summary_controls_rel['change_in_mean_accuracy'], \n",
    "    summary_patients_rel['change_in_mean_accuracy'], \n",
    "    'change_in_mean_accuracy', \n",
    "    ' ', \n",
    "    'Accuracy Delta',\n",
    "    colors = {0: 'olive', 1: 'darkblue'},\n",
    "    ylims = (-0.7, 0.4)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e4919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confidence \n",
    "plot_change_in_mean_var(\n",
    "    summary_controls_rel['change_in_mean_confidence'], \n",
    "    summary_patients_rel['change_in_mean_confidence'], \n",
    "    'change_in_mean_confidence', \n",
    "    ' ',\n",
    "    'Confidence Delta',\n",
    "     colors = {0: '#16463F', 1: '#2D80A7'},\n",
    "     ylims = (-19,6)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
